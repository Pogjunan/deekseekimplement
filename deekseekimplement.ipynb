{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99887bbd-c468-43d4-b470-96516db178b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code 1:\n",
      "<|fim▁begin|>\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "\n",
      "\n",
      "def fibonacci(n):\n",
      "    \"\"\"n개의 피보나치 수열을 반환합니다.\"\"\"\n",
      "    if n <= 0:\n",
      "        return []\n",
      "    elif n == 1:\n",
      "        return [0]\n",
      "    seq = [0, 1]\n",
      "    <|fim▁hole|>  # 모델이 while 루프를 채워서 나머지 피보나치 수열을 생성하도록 유도\n",
      "<|fim▁end|>\n",
      "\n",
      "\n",
      "Code 2:\n",
      "<|fim▁begin|>class MathOperations:\n",
      "    @staticmethod\n",
      "    def multiply(a, b):\n",
      "        return a * b\n",
      "\n",
      "    @staticmethod\n",
      "    def divide(a, b):\n",
      "        if b == 0:\n",
      "            raise ValueError(\"Cannot divide by zero\")\n",
      "        return a / b\n",
      "\n",
      "\n",
      "def matrix_multiply(A, B):\n",
      "    \"\"\"\n",
      "    두 행렬 A와 B의 곱을 계산합니다.\n",
      "    A: m x n 행렬, B: n x p 행렬\n",
      "    반환값: m x p 행렬\n",
      "    \"\"\"\n",
      "    m = len(A)\n",
      "    n = len(A[0])\n",
      "    p = len(B[0])\n",
      "    \n",
      "    # 결과 행렬을 0으로 초기화\n",
      "    result = [[0 for _ in range(p)] for _ in range(m)]\n",
      "    \n",
      "    <|fim▁hole|>\n",
      "    \n",
      "    return result\n",
      "<|fim▁end|>\n",
      "\n",
      "\n",
      "Code 3:\n",
      "<|fim▁begin|>\n",
      "class Calculator:\n",
      "    def __init__(self):\n",
      "        self.result = 0\n",
      "\n",
      "    def add(self, a, b):\n",
      "        self.result = a + b\n",
      "        return self.result\n",
      "\n",
      "    def subtract(self, a, b):\n",
      "        self.result = a - b\n",
      "        return self.result\n",
      "\n",
      "\n",
      "def binary_search(arr, target):\n",
      "    \"\"\"정렬된 리스트에서 target의 인덱스를 이진 탐색으로 찾습니다.\n",
      "       target이 없으면 -1을 반환합니다.\n",
      "    \"\"\"\n",
      "    low, high = 0, len(arr) - 1\n",
      "    <|fim▁hole|>  # 모델이 while 루프와 조건문을 생성하여 이진 탐색을 완성\n",
      "<|fim▁end|>\n",
      "\n",
      "\n",
      "Code 4:\n",
      "<|fim▁begin|>\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def subtract(a, b):\n",
      "    return a - b\n",
      "\n",
      "\n",
      "def merge_sort(arr):\n",
      "    \"\"\"리스트를 병합 정렬(merge sort) 방식으로 정렬합니다.\"\"\"\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    mid = len(arr) // 2\n",
      "    left = merge_sort(arr[:mid])\n",
      "    right = merge_sort(arr[mid:])\n",
      "    <|fim▁hole|>  # 모델이 여기서 merge(left, right) 호출 또는 적절한 코드를 생성\n",
      "       \n",
      "def merge(left, right):\n",
      "    \"\"\"두 정렬된 리스트를 병합합니다.\"\"\"\n",
      "    merged = []\n",
      "    i = j = 0\n",
      "    while i < len(left) and j < len(right):\n",
      "        if left[i] < right[j]:\n",
      "            merged.append(left[i])\n",
      "            i += 1\n",
      "        else:\n",
      "            merged.append(right[j])\n",
      "            j += 1\n",
      "    merged.extend(left[i:])\n",
      "    merged.extend(right[j:])\n",
      "    return merged\n",
      "<|fim▁end|>\n",
      "\n",
      "\n",
      "Code 5:\n",
      "<|fim▁begin|>\n",
      "def is_prime(n):\n",
      "    \"\"\"n이 소수이면 True, 아니면 False를 반환합니다.\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    <|fim▁hole|>  # 모델이 while 루프를 완성하여 소수 판별 논리를 생성\n",
      "<|fim▁end|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_data(data_dir):\n",
    "    code_files = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith('.py')]\n",
    "    codes = []\n",
    "    for file in code_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            codes.append(f.read())\n",
    "    return codes\n",
    "\n",
    "data_dir = 'sample_data'\n",
    "codes = load_data(data_dir)\n",
    "\n",
    "for i, code in enumerate(codes):\n",
    "    print(f\"Code {i+1}:\\n{code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d154898-856c-42b1-84ce-6b0c70d47e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|fim▁begin|>\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\n\\ndef fibonacci(n):\\n    \"\"\"n개의 피보나치 수열을 반환합니다.\"\"\"\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    seq = [0, 1]\\n    <|fim▁hole|>  # 모델이 while 루프를 채워서 나머지 피보나치 수열을 생성하도록 유도\\n<|fim▁end|>\\n',\n",
       " '<|fim▁begin|>class MathOperations:\\n    @staticmethod\\n    def multiply(a, b):\\n        return a * b\\n\\n    @staticmethod\\n    def divide(a, b):\\n        if b == 0:\\n            raise ValueError(\"Cannot divide by zero\")\\n        return a / b\\n\\n\\ndef matrix_multiply(A, B):\\n    \"\"\"\\n    두 행렬 A와 B의 곱을 계산합니다.\\n    A: m x n 행렬, B: n x p 행렬\\n    반환값: m x p 행렬\\n    \"\"\"\\n    m = len(A)\\n    n = len(A[0])\\n    p = len(B[0])\\n    \\n    # 결과 행렬을 0으로 초기화\\n    result = [[0 for _ in range(p)] for _ in range(m)]\\n    \\n    <|fim▁hole|>\\n    \\n    return result\\n<|fim▁end|>\\n',\n",
       " '<|fim▁begin|>\\nclass Calculator:\\n    def __init__(self):\\n        self.result = 0\\n\\n    def add(self, a, b):\\n        self.result = a + b\\n        return self.result\\n\\n    def subtract(self, a, b):\\n        self.result = a - b\\n        return self.result\\n\\n\\ndef binary_search(arr, target):\\n    \"\"\"정렬된 리스트에서 target의 인덱스를 이진 탐색으로 찾습니다.\\n       target이 없으면 -1을 반환합니다.\\n    \"\"\"\\n    low, high = 0, len(arr) - 1\\n    <|fim▁hole|>  # 모델이 while 루프와 조건문을 생성하여 이진 탐색을 완성\\n<|fim▁end|>\\n',\n",
       " '<|fim▁begin|>\\ndef add(a, b):\\n    return a + b\\n\\ndef subtract(a, b):\\n    return a - b\\n\\n\\ndef merge_sort(arr):\\n    \"\"\"리스트를 병합 정렬(merge sort) 방식으로 정렬합니다.\"\"\"\\n    if len(arr) <= 1:\\n        return arr\\n    mid = len(arr) // 2\\n    left = merge_sort(arr[:mid])\\n    right = merge_sort(arr[mid:])\\n    <|fim▁hole|>  # 모델이 여기서 merge(left, right) 호출 또는 적절한 코드를 생성\\n       \\ndef merge(left, right):\\n    \"\"\"두 정렬된 리스트를 병합합니다.\"\"\"\\n    merged = []\\n    i = j = 0\\n    while i < len(left) and j < len(right):\\n        if left[i] < right[j]:\\n            merged.append(left[i])\\n            i += 1\\n        else:\\n            merged.append(right[j])\\n            j += 1\\n    merged.extend(left[i:])\\n    merged.extend(right[j:])\\n    return merged\\n<|fim▁end|>\\n',\n",
       " '<|fim▁begin|>\\ndef is_prime(n):\\n    \"\"\"n이 소수이면 True, 아니면 False를 반환합니다.\"\"\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    <|fim▁hole|>  # 모델이 while 루프를 완성하여 소수 판별 논리를 생성\\n<|fim▁end|>\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b12c7e0c-fca2-4f8f-a4c9-2a4b7b149a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# eos_token을 패딩 토큰으로 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 코드 토큰화\n",
    "tokenized_codes = [tokenizer(code, return_tensors='pt', truncation=True, padding=True) for code in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba7ee7a6-6cdb-4910-9b21-36ed0e785b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Code 1:\n",
      "{'input_ids': tensor([[  198,  4299,  1109,  5132,     7,    77,  2599,   198,   220,   220,\n",
      "           220,   611,   299,  6624,   657,    25,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,   352,   198,   220,   220,   220,\n",
      "          2073,    25,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1441,   299,  1635,  1109,  5132,     7,    77,   532,   352,     8,\n",
      "           628,   198,  4299, 12900,   261, 44456,     7,    77,  2599,   198,\n",
      "           220,   220,   220, 37227,    77,   166,   108,   250, 35975,   246,\n",
      "           220,   169,   242,   120,   167,   111,   112,   167,   224,   246,\n",
      "           168,   117,   246, 23821,   230,   246,   168,   245,   112, 35975,\n",
      "           226, 31619,   108,   246,   169,   247,   246, 47991,   102, 46695,\n",
      "           230, 46695,    97,   526, 15931,   198,   220,   220,   220,   611,\n",
      "           299, 19841,   657,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441, 17635,   198,   220,   220,   220,  1288,   361,\n",
      "           299,  6624,   352,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441,   685,    15,    60,   198,   220,   220,   220,\n",
      "         33756,   796,   685,    15,    11,   352,    60,   198,   220,   220,\n",
      "           220,   981, 18896,     7, 41068,     8,  1279,   299,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220, 33756,    13, 33295,\n",
      "             7, 41068,    58,    12,    16,    60,  1343, 33756,    58,    12,\n",
      "            17, 12962,   198,   220,   220,   220,  1441, 33756,   198,   198,\n",
      "             2, 23821,  8955,   168,   248,   102, 23821,   246,   230,   168,\n",
      "           233,   250,    25,   198,     2,   361, 11593,  3672,   834,  6624,\n",
      "           366,   834, 12417,   834,  1298,   198,     2,   220,   220,   220,\n",
      "           299,    62, 38707,   796,   838,   198,     2,   220,   220,   220,\n",
      "          3601,  7203,    37,   571,   261, 44456, 23821,   230,   246,   168,\n",
      "           245,   112,    25,  1600, 12900,   261, 44456,     7,    77,    62,\n",
      "         38707,  4008,   628]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 2:\n",
      "{'input_ids': tensor([[  198,  4871, 16320, 18843,   602,    25,   198,   220,   220,   220,\n",
      "          2488, 12708, 24396,   198,   220,   220,   220,   825, 29162,     7,\n",
      "            64,    11,   275,  2599,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441,   257,  1635,   275,   628,   220,   220,   220,\n",
      "          2488, 12708, 24396,   198,   220,   220,   220,   825, 14083,     7,\n",
      "            64,    11,   275,  2599,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   611,   275,  6624,   657,    25,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,  5298,\n",
      "         11052, 12331,  7203,    34, 34574, 14083,   416,  6632,  4943,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  1441,   257,  1220,\n",
      "           275,   628,   198,  4299, 17593,    62, 16680,   541,   306,     7,\n",
      "            32,    11,   347,  2599,   198,   220,   220,   220, 37227,   198,\n",
      "           220,   220,   220, 31619,   239,   238,   220,   169,   244,   231,\n",
      "           167,   254,   105,   317,   168,   247,   222,   347, 35975,   246,\n",
      "           220,   166,   111,   109, 35975,   226,   220,   166,   111,   226,\n",
      "           168,   224,   108, 47991,   102, 46695,   230, 46695,    97,    13,\n",
      "           198,   220,   220,   220,   317,    25,   285,  2124,   299,   220,\n",
      "           169,   244,   231,   167,   254,   105,    11,   347,    25,   299,\n",
      "          2124,   279,   220,   169,   244,   231,   167,   254,   105,   198,\n",
      "           220,   220,   220, 31619,   108,   246,   169,   247,   246,   166,\n",
      "           108,   240,    25,   285,  2124,   279,   220,   169,   244,   231,\n",
      "           167,   254,   105,   198,   220,   220,   220, 37227,   198,   220,\n",
      "           220,   220,   285,   796, 18896,     7,    32,     8,   198,   220,\n",
      "           220,   220,   299,   796, 18896,     7,    32,    58,    15, 12962,\n",
      "           198,   220,   220,   220,   279,   796, 18896,     7,    33,    58,\n",
      "            15, 12962,   198,   220,   220,   220,   220,   198,   220,   220,\n",
      "           220,  1303,   220,   166,   110,   108,   166,   111,   120,   220,\n",
      "           169,   244,   231,   167,   254,   105, 35975,   226,   657,   168,\n",
      "           250,   120,   167,    94,   250, 23821,   112,   230,   166,   116,\n",
      "           108,   169,   247,   242,   198,   220,   220,   220,  1255,   796,\n",
      "         16410,    15,   329,  4808,   287,  2837,     7,    79, 15437,   329,\n",
      "          4808,   287,  2837,     7,    76, 15437,   198,   220,   220,   220,\n",
      "           220,   198,   220,   220,   220,   329,  1312,   287,  2837,     7,\n",
      "            76,  2599,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "           329,   474,   287,  2837,     7,    79,  2599,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   329,\n",
      "           479,   287,  2837,     7,    77,  2599,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1255,    58,    72,  7131,    73,    60, 15853,   317,\n",
      "            58,    72,  7131,    74,    60,  1635,   347,    58,    74,  7131,\n",
      "            73,    60,   198,   220,   220,   220,  1441,  1255,   198,   198,\n",
      "             2, 23821,  8955,   168,   248,   102, 23821,   246,   230,   168,\n",
      "           233,   250,    25,   198,     2,   361, 11593,  3672,   834,  6624,\n",
      "           366,   834, 12417,   834,  1298,   198,     2,   220,   220,   220,\n",
      "           317,   796, 16410,    16,    11,   362,  4357,   685,    18,    11,\n",
      "           604, 11907,   198,     2,   220,   220,   220,   347,   796, 16410,\n",
      "            20,    11,   718,  4357,   685,    22,    11,   807, 11907,   198,\n",
      "             2,   220,   220,   220,  3601,  7203, 46912, 15237,   489,  3299,\n",
      "           220,   166,   110,   108,   166,   111,   120,    25,  1600, 17593,\n",
      "            62, 16680,   541,   306,     7,    32,    11,   347,  4008,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 3:\n",
      "{'input_ids': tensor([[  198,  4871, 43597,    25,   198,   220,   220,   220,   825, 11593,\n",
      "         15003,   834,     7,   944,  2599,   198,   220,   220,   220,   220,\n",
      "           220,   220,   220,  2116,    13, 20274,   796,   657,   628,   220,\n",
      "           220,   220,   825,   751,     7,   944,    11,   257,    11,   275,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,  2116,\n",
      "            13, 20274,   796,   257,  1343,   275,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  2116,    13, 20274,   628,   220,\n",
      "           220,   220,   825, 34128,     7,   944,    11,   257,    11,   275,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,  2116,\n",
      "            13, 20274,   796,   257,   532,   275,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  2116,    13, 20274,   628,   198,\n",
      "          4299, 13934,    62, 12947,     7,  3258,    11,  2496,  2599,   198,\n",
      "           220,   220,   220, 37227,   168,   254,   243,   167,   254,   105,\n",
      "           167,   238,   250, 31619,    99,   105,   168,   232,    97,   169,\n",
      "           232,   116,   168,   245,   238,   168,   226,   250,  2496, 35975,\n",
      "           246, 23821,   251,   116,   167,   235,   109,   168,   232,    97,\n",
      "           167,    98,   120, 23821,   251,   112,   168,   100,   226,   220,\n",
      "           169,   225,   238,   168,   225,   231,   168,   250,   120,   167,\n",
      "            94,   250, 23821,   108,   122,   168,   232,   113, 46695,   230,\n",
      "         46695,    97,    13,   198,   220,   220,   220,   220,   220,   220,\n",
      "          2496, 35975,   112, 23821,   245,   228,   168,   250,   120,   167,\n",
      "           102,   112,   532,    16, 35975,   226, 31619,   108,   246,   169,\n",
      "           247,   246, 47991,   102, 46695,   230, 46695,    97,    13,   198,\n",
      "           220,   220,   220, 37227,   198,   220,   220,   220,  1877,    11,\n",
      "          1029,   796,   657,    11, 18896,     7,  3258,     8,   532,   352,\n",
      "           198,   220,   220,   220,   981,  1877, 19841,  1029,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  3095,   796,   357,\n",
      "          9319,  1343,  1029,     8,  3373,   362,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,   611,  5240,    58, 13602,    60,  6624,\n",
      "          2496,    25,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  3095,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1288,   361,  5240,    58, 13602,    60,\n",
      "          1279,  2496,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,  1877,   796,  3095,  1343,   352,\n",
      "           198,   220,   220,   220,   220,   220,   220,   220,  2073,    25,\n",
      "           198,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1029,   796,  3095,   532,   352,   198,   220,   220,\n",
      "           220,  1441,   532,    16,   198,   198,     2, 23821,  8955,   168,\n",
      "           248,   102, 23821,   246,   230,   168,   233,   250,    25,   198,\n",
      "             2,   361, 11593,  3672,   834,  6624,   366,   834, 12417,   834,\n",
      "          1298,   198,     2,   220,   220,   220, 23243,    62,  4868,   796,\n",
      "           685,    18,    11,   767,    11,  1105,    11,   678,    11,  1679,\n",
      "            11,  3261,    60,   198,     2,   220,   220,   220,  2496,   796,\n",
      "           678,   198,     2,   220,   220,   220,  3601,  7203,    33,  3219,\n",
      "         11140,   220,   166,   110,   108,   166,   111,   120,    25,  1600,\n",
      "         13934,    62, 12947,     7,    82,  9741,    62,  4868,    11,  2496,\n",
      "          4008,   628]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 4:\n",
      "{'input_ids': tensor([[  198,  4299,   751,     7,    64,    11,   275,  2599,   198,   220,\n",
      "           220,   220,  1441,   257,  1343,   275,   198,   198,  4299, 34128,\n",
      "             7,    64,    11,   275,  2599,   198,   220,   220,   220,  1441,\n",
      "           257,   532,   275,   628,   198,  4299, 20121,    62, 30619,     7,\n",
      "          3258,  2599,   198,   220,   220,   220, 37227,   167,    99,   105,\n",
      "           168,   232,    97,   169,   232,   116,   167,    98,   120, 31619,\n",
      "           111,   239, 47991,   102, 23821,   254,   243,   167,   254,   105,\n",
      "             7,   647,   469,  3297,     8, 31619,   108,   102,   168,   233,\n",
      "           251,   168,   250,   120,   167,    94,   250, 23821,   254,   243,\n",
      "           167,   254,   105, 47991,   102, 46695,   230, 46695,    97,   526,\n",
      "         15931,   198,   220,   220,   220,   611, 18896,     7,  3258,     8,\n",
      "         19841,   352,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,  1441,  5240,   198,   220,   220,   220,  3095,   796, 18896,\n",
      "             7,  3258,     8,  3373,   362,   198,   220,   220,   220,  1364,\n",
      "           796, 20121,    62, 30619,     7,  3258,    58,    25, 13602, 12962,\n",
      "           198,   220,   220,   220,   826,   796, 20121,    62, 30619,     7,\n",
      "          3258,    58, 13602,    25, 12962,   198,   220,   220,   220,  1441,\n",
      "         20121,     7,  9464,    11,   826,     8,   198,   198,  4299, 20121,\n",
      "             7,  9464,    11,   826,  2599,   198,   220,   220,   220, 37227,\n",
      "           167,   239,   238, 23821,   254,   243,   167,   254,   105,   167,\n",
      "           238,   250, 31619,    99,   105,   168,   232,    97,   169,   232,\n",
      "           116,   167,    98,   120, 31619,   111,   239, 47991,   102, 47991,\n",
      "           102, 46695,   230, 46695,    97,   526, 15931,   198,   220,   220,\n",
      "           220, 23791,   796, 17635,   198,   220,   220,   220,  1312,   796,\n",
      "           474,   796,   657,   198,   220,   220,   220,   981,  1312,  1279,\n",
      "         18896,     7,  9464,     8,   290,   474,  1279, 18896,     7,  3506,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,   611,\n",
      "          1364,    58,    72,    60,  1279,   826,    58,    73,  5974,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220, 23791,    13, 33295,     7,  9464,    58,    72, 12962,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,  1312, 15853,   352,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  2073,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220, 23791,    13, 33295,     7,\n",
      "          3506,    58,    73, 12962,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   474, 15853,   352,   198,\n",
      "           220,   220,   220, 23791,    13,  2302,   437,     7,  9464,    58,\n",
      "            72,    25, 12962,   198,   220,   220,   220, 23791,    13,  2302,\n",
      "           437,     7,  3506,    58,    73,    25, 12962,   198,   220,   220,\n",
      "           220,  1441, 23791,   198,   198,     2, 23821,  8955,   168,   248,\n",
      "           102, 23821,   246,   230,   168,   233,   250,    25,   198,     2,\n",
      "           361, 11593,  3672,   834,  6624,   366,   834, 12417,   834,  1298,\n",
      "           198,     2,   220,   220,   220,  6291,    62,  4868,   796,   685,\n",
      "          2682,    11,   767,    11,  2242,    11,  3933,    11,   642,    11,\n",
      "          8190,    60,   198,     2,   220,   220,   220,  3601,  7203, 13102,\n",
      "           469, 33947,   220,   166,   110,   108,   166,   111,   120,    25,\n",
      "          1600, 20121,    62, 30619,     7, 39873,    62,  4868,  4008,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 5:\n",
      "{'input_ids': tensor([[  198,  4299,   318,    62, 35505,     7,    77,  2599,   198,   220,\n",
      "           220,   220,   611,   299, 19841,   352,    25,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,  1441, 10352,   198,   220,   220,\n",
      "           220,   329,  1312,   287,  2837,     7,    17,    11,   493,     7,\n",
      "            77,  1174,    15,    13,    20,     8,  1343,   352,  2599,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   611,   299,  4064,\n",
      "          1312,  6624,   657,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,  1441, 10352,   198,   220,\n",
      "           220,   220,  1441,  6407,   628,   198,  4299,   318,    62, 35505,\n",
      "             7,    77,  2599,   198,   220,   220,   220, 37227,    77, 35975,\n",
      "           112, 23821,   228,   234,   168,   230,   246, 35975,   112,   167,\n",
      "           102,   112,  6407,    11, 23821,   243,   226, 46695,   230,   167,\n",
      "           102,   112, 10352,   167,    98,   120, 31619,   108,   246,   169,\n",
      "           247,   246, 47991,   102, 46695,   230, 46695,    97,   526, 15931,\n",
      "           198,   220,   220,   220,   611,   299, 19841,   352,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  1441, 10352,   198,\n",
      "           220,   220,   220,   611,   299, 19841,   513,    25,   198,   220,\n",
      "           220,   220,   220,   220,   220,   220,  1441,  6407,   198,   220,\n",
      "           220,   220,   611,   299,  4064,   362,  6624,   657,   393,   299,\n",
      "          4064,   513,  6624,   657,    25,   198,   220,   220,   220,   220,\n",
      "           220,   220,   220,  1441, 10352,   198,   220,   220,   220,  1312,\n",
      "           796,   642,   198,   220,   220,   220,   981,  1312,  1635,  1312,\n",
      "         19841,   299,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,   611,   299,  4064,  1312,  6624,   657,   393,   299,  4064,\n",
      "           357,    72,  1343,   362,     8,  6624,   657,    25,   198,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1441, 10352,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1312, 15853,   718,   198,   220,   220,   220,  1441,  6407,   198,\n",
      "           198,     2, 23821,  8955,   168,   248,   102, 23821,   246,   230,\n",
      "           168,   233,   250,    25,   198,     2,   361, 11593,  3672,   834,\n",
      "          6624,   366,   834, 12417,   834,  1298,   198,     2,   220,   220,\n",
      "           220,   997,   796,  2808,   198,     2,   220,   220,   220,  3601,\n",
      "             7,    69,     1,    90, 22510,    92,   167,   232,   242, 23821,\n",
      "           228,   234,   168,   230,   246, 35975,   116,   166,   108,   222,\n",
      "           168,   248,   242,    30,  1391,   271,    62, 35505,     7, 22510,\n",
      "         38165,  4943,   628,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "for i, tokenized_code in enumerate(tokenized_codes):\n",
    "    print(f\"Tokenized Code {i+1}:\")\n",
    "    print(tokenized_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468d8cb5-a980-4260-87cb-f6ecc869f883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Code 1:\n",
      "{'input_ids': tensor([[  198,  4299,  1109,  5132,     7,    77,  2599,   198,   220,   220,\n",
      "           220,   611,   299,  6624,   657,    25,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,   352,   198,   220,   220,   220,\n",
      "          2073,    25,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1441,   299,  1635,  1109,  5132,     7,    77,   532,   352,     8,\n",
      "           628,   198,  4299, 12900,   261, 44456,     7,    77,  2599,   198,\n",
      "           220,   220,   220, 37227,    77,   166,   108,   250, 35975,   246,\n",
      "           220,   169,   242,   120,   167,   111,   112,   167,   224,   246,\n",
      "           168,   117,   246, 23821,   230,   246,   168,   245,   112, 35975,\n",
      "           226, 31619,   108,   246,   169,   247,   246, 47991,   102, 46695,\n",
      "           230, 46695,    97,   526, 15931,   198,   220,   220,   220,   611,\n",
      "           299, 19841,   657,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441, 17635,   198,   220,   220,   220,  1288,   361,\n",
      "           299,  6624,   352,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441,   685,    15,    60,   198,   220,   220,   220,\n",
      "         33756,   796,   685,    15,    11,   352,    60,   198,   220,   220,\n",
      "           220,   981, 18896,     7, 41068,     8,  1279,   299,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220, 33756,    13, 33295,\n",
      "             7, 41068,    58,    12,    16,    60,  1343, 33756,    58,    12,\n",
      "            17, 12962,   198,   220,   220,   220,  1441, 33756,   198,   198,\n",
      "             2, 23821,  8955,   168,   248,   102, 23821,   246,   230,   168,\n",
      "           233,   250,    25,   198,     2,   361, 11593,  3672,   834,  6624,\n",
      "           366,   834, 12417,   834,  1298,   198,     2,   220,   220,   220,\n",
      "           299,    62, 38707,   796,   838,   198,     2,   220,   220,   220,\n",
      "          3601,  7203,    37,   571,   261, 44456, 23821,   230,   246,   168,\n",
      "           245,   112,    25,  1600, 12900,   261, 44456,     7,    77,    62,\n",
      "         38707,  4008,   628]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 2:\n",
      "{'input_ids': tensor([[  198,  4871, 16320, 18843,   602,    25,   198,   220,   220,   220,\n",
      "          2488, 12708, 24396,   198,   220,   220,   220,   825, 29162,     7,\n",
      "            64,    11,   275,  2599,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1441,   257,  1635,   275,   628,   220,   220,   220,\n",
      "          2488, 12708, 24396,   198,   220,   220,   220,   825, 14083,     7,\n",
      "            64,    11,   275,  2599,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   611,   275,  6624,   657,    25,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,  5298,\n",
      "         11052, 12331,  7203,    34, 34574, 14083,   416,  6632,  4943,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  1441,   257,  1220,\n",
      "           275,   628,   198,  4299, 17593,    62, 16680,   541,   306,     7,\n",
      "            32,    11,   347,  2599,   198,   220,   220,   220, 37227,   198,\n",
      "           220,   220,   220, 31619,   239,   238,   220,   169,   244,   231,\n",
      "           167,   254,   105,   317,   168,   247,   222,   347, 35975,   246,\n",
      "           220,   166,   111,   109, 35975,   226,   220,   166,   111,   226,\n",
      "           168,   224,   108, 47991,   102, 46695,   230, 46695,    97,    13,\n",
      "           198,   220,   220,   220,   317,    25,   285,  2124,   299,   220,\n",
      "           169,   244,   231,   167,   254,   105,    11,   347,    25,   299,\n",
      "          2124,   279,   220,   169,   244,   231,   167,   254,   105,   198,\n",
      "           220,   220,   220, 31619,   108,   246,   169,   247,   246,   166,\n",
      "           108,   240,    25,   285,  2124,   279,   220,   169,   244,   231,\n",
      "           167,   254,   105,   198,   220,   220,   220, 37227,   198,   220,\n",
      "           220,   220,   285,   796, 18896,     7,    32,     8,   198,   220,\n",
      "           220,   220,   299,   796, 18896,     7,    32,    58,    15, 12962,\n",
      "           198,   220,   220,   220,   279,   796, 18896,     7,    33,    58,\n",
      "            15, 12962,   198,   220,   220,   220,   220,   198,   220,   220,\n",
      "           220,  1303,   220,   166,   110,   108,   166,   111,   120,   220,\n",
      "           169,   244,   231,   167,   254,   105, 35975,   226,   657,   168,\n",
      "           250,   120,   167,    94,   250, 23821,   112,   230,   166,   116,\n",
      "           108,   169,   247,   242,   198,   220,   220,   220,  1255,   796,\n",
      "         16410,    15,   329,  4808,   287,  2837,     7,    79, 15437,   329,\n",
      "          4808,   287,  2837,     7,    76, 15437,   198,   220,   220,   220,\n",
      "           220,   198,   220,   220,   220,   329,  1312,   287,  2837,     7,\n",
      "            76,  2599,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "           329,   474,   287,  2837,     7,    79,  2599,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   329,\n",
      "           479,   287,  2837,     7,    77,  2599,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1255,    58,    72,  7131,    73,    60, 15853,   317,\n",
      "            58,    72,  7131,    74,    60,  1635,   347,    58,    74,  7131,\n",
      "            73,    60,   198,   220,   220,   220,  1441,  1255,   198,   198,\n",
      "             2, 23821,  8955,   168,   248,   102, 23821,   246,   230,   168,\n",
      "           233,   250,    25,   198,     2,   361, 11593,  3672,   834,  6624,\n",
      "           366,   834, 12417,   834,  1298,   198,     2,   220,   220,   220,\n",
      "           317,   796, 16410,    16,    11,   362,  4357,   685,    18,    11,\n",
      "           604, 11907,   198,     2,   220,   220,   220,   347,   796, 16410,\n",
      "            20,    11,   718,  4357,   685,    22,    11,   807, 11907,   198,\n",
      "             2,   220,   220,   220,  3601,  7203, 46912, 15237,   489,  3299,\n",
      "           220,   166,   110,   108,   166,   111,   120,    25,  1600, 17593,\n",
      "            62, 16680,   541,   306,     7,    32,    11,   347,  4008,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 3:\n",
      "{'input_ids': tensor([[  198,  4871, 43597,    25,   198,   220,   220,   220,   825, 11593,\n",
      "         15003,   834,     7,   944,  2599,   198,   220,   220,   220,   220,\n",
      "           220,   220,   220,  2116,    13, 20274,   796,   657,   628,   220,\n",
      "           220,   220,   825,   751,     7,   944,    11,   257,    11,   275,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,  2116,\n",
      "            13, 20274,   796,   257,  1343,   275,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  2116,    13, 20274,   628,   220,\n",
      "           220,   220,   825, 34128,     7,   944,    11,   257,    11,   275,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,  2116,\n",
      "            13, 20274,   796,   257,   532,   275,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  2116,    13, 20274,   628,   198,\n",
      "          4299, 13934,    62, 12947,     7,  3258,    11,  2496,  2599,   198,\n",
      "           220,   220,   220, 37227,   168,   254,   243,   167,   254,   105,\n",
      "           167,   238,   250, 31619,    99,   105,   168,   232,    97,   169,\n",
      "           232,   116,   168,   245,   238,   168,   226,   250,  2496, 35975,\n",
      "           246, 23821,   251,   116,   167,   235,   109,   168,   232,    97,\n",
      "           167,    98,   120, 23821,   251,   112,   168,   100,   226,   220,\n",
      "           169,   225,   238,   168,   225,   231,   168,   250,   120,   167,\n",
      "            94,   250, 23821,   108,   122,   168,   232,   113, 46695,   230,\n",
      "         46695,    97,    13,   198,   220,   220,   220,   220,   220,   220,\n",
      "          2496, 35975,   112, 23821,   245,   228,   168,   250,   120,   167,\n",
      "           102,   112,   532,    16, 35975,   226, 31619,   108,   246,   169,\n",
      "           247,   246, 47991,   102, 46695,   230, 46695,    97,    13,   198,\n",
      "           220,   220,   220, 37227,   198,   220,   220,   220,  1877,    11,\n",
      "          1029,   796,   657,    11, 18896,     7,  3258,     8,   532,   352,\n",
      "           198,   220,   220,   220,   981,  1877, 19841,  1029,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  3095,   796,   357,\n",
      "          9319,  1343,  1029,     8,  3373,   362,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,   611,  5240,    58, 13602,    60,  6624,\n",
      "          2496,    25,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1441,  3095,   198,   220,   220,   220,\n",
      "           220,   220,   220,   220,  1288,   361,  5240,    58, 13602,    60,\n",
      "          1279,  2496,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,  1877,   796,  3095,  1343,   352,\n",
      "           198,   220,   220,   220,   220,   220,   220,   220,  2073,    25,\n",
      "           198,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,  1029,   796,  3095,   532,   352,   198,   220,   220,\n",
      "           220,  1441,   532,    16,   198,   198,     2, 23821,  8955,   168,\n",
      "           248,   102, 23821,   246,   230,   168,   233,   250,    25,   198,\n",
      "             2,   361, 11593,  3672,   834,  6624,   366,   834, 12417,   834,\n",
      "          1298,   198,     2,   220,   220,   220, 23243,    62,  4868,   796,\n",
      "           685,    18,    11,   767,    11,  1105,    11,   678,    11,  1679,\n",
      "            11,  3261,    60,   198,     2,   220,   220,   220,  2496,   796,\n",
      "           678,   198,     2,   220,   220,   220,  3601,  7203,    33,  3219,\n",
      "         11140,   220,   166,   110,   108,   166,   111,   120,    25,  1600,\n",
      "         13934,    62, 12947,     7,    82,  9741,    62,  4868,    11,  2496,\n",
      "          4008,   628]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 4:\n",
      "{'input_ids': tensor([[  198,  4299,   751,     7,    64,    11,   275,  2599,   198,   220,\n",
      "           220,   220,  1441,   257,  1343,   275,   198,   198,  4299, 34128,\n",
      "             7,    64,    11,   275,  2599,   198,   220,   220,   220,  1441,\n",
      "           257,   532,   275,   628,   198,  4299, 20121,    62, 30619,     7,\n",
      "          3258,  2599,   198,   220,   220,   220, 37227,   167,    99,   105,\n",
      "           168,   232,    97,   169,   232,   116,   167,    98,   120, 31619,\n",
      "           111,   239, 47991,   102, 23821,   254,   243,   167,   254,   105,\n",
      "             7,   647,   469,  3297,     8, 31619,   108,   102,   168,   233,\n",
      "           251,   168,   250,   120,   167,    94,   250, 23821,   254,   243,\n",
      "           167,   254,   105, 47991,   102, 46695,   230, 46695,    97,   526,\n",
      "         15931,   198,   220,   220,   220,   611, 18896,     7,  3258,     8,\n",
      "         19841,   352,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,  1441,  5240,   198,   220,   220,   220,  3095,   796, 18896,\n",
      "             7,  3258,     8,  3373,   362,   198,   220,   220,   220,  1364,\n",
      "           796, 20121,    62, 30619,     7,  3258,    58,    25, 13602, 12962,\n",
      "           198,   220,   220,   220,   826,   796, 20121,    62, 30619,     7,\n",
      "          3258,    58, 13602,    25, 12962,   198,   220,   220,   220,  1441,\n",
      "         20121,     7,  9464,    11,   826,     8,   198,   198,  4299, 20121,\n",
      "             7,  9464,    11,   826,  2599,   198,   220,   220,   220, 37227,\n",
      "           167,   239,   238, 23821,   254,   243,   167,   254,   105,   167,\n",
      "           238,   250, 31619,    99,   105,   168,   232,    97,   169,   232,\n",
      "           116,   167,    98,   120, 31619,   111,   239, 47991,   102, 47991,\n",
      "           102, 46695,   230, 46695,    97,   526, 15931,   198,   220,   220,\n",
      "           220, 23791,   796, 17635,   198,   220,   220,   220,  1312,   796,\n",
      "           474,   796,   657,   198,   220,   220,   220,   981,  1312,  1279,\n",
      "         18896,     7,  9464,     8,   290,   474,  1279, 18896,     7,  3506,\n",
      "          2599,   198,   220,   220,   220,   220,   220,   220,   220,   611,\n",
      "          1364,    58,    72,    60,  1279,   826,    58,    73,  5974,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220, 23791,    13, 33295,     7,  9464,    58,    72, 12962,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,  1312, 15853,   352,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,  2073,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220, 23791,    13, 33295,     7,\n",
      "          3506,    58,    73, 12962,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   474, 15853,   352,   198,\n",
      "           220,   220,   220, 23791,    13,  2302,   437,     7,  9464,    58,\n",
      "            72,    25, 12962,   198,   220,   220,   220, 23791,    13,  2302,\n",
      "           437,     7,  3506,    58,    73,    25, 12962,   198,   220,   220,\n",
      "           220,  1441, 23791,   198,   198,     2, 23821,  8955,   168,   248,\n",
      "           102, 23821,   246,   230,   168,   233,   250,    25,   198,     2,\n",
      "           361, 11593,  3672,   834,  6624,   366,   834, 12417,   834,  1298,\n",
      "           198,     2,   220,   220,   220,  6291,    62,  4868,   796,   685,\n",
      "          2682,    11,   767,    11,  2242,    11,  3933,    11,   642,    11,\n",
      "          8190,    60,   198,     2,   220,   220,   220,  3601,  7203, 13102,\n",
      "           469, 33947,   220,   166,   110,   108,   166,   111,   120,    25,\n",
      "          1600, 20121,    62, 30619,     7, 39873,    62,  4868,  4008,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokenized Code 5:\n",
      "{'input_ids': tensor([[  198,  4299,   318,    62, 35505,     7,    77,  2599,   198,   220,\n",
      "           220,   220,   611,   299, 19841,   352,    25,   198,   220,   220,\n",
      "           220,   220,   220,   220,   220,  1441, 10352,   198,   220,   220,\n",
      "           220,   329,  1312,   287,  2837,     7,    17,    11,   493,     7,\n",
      "            77,  1174,    15,    13,    20,     8,  1343,   352,  2599,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,   611,   299,  4064,\n",
      "          1312,  6624,   657,    25,   198,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,  1441, 10352,   198,   220,\n",
      "           220,   220,  1441,  6407,   628,   198,  4299,   318,    62, 35505,\n",
      "             7,    77,  2599,   198,   220,   220,   220, 37227,    77, 35975,\n",
      "           112, 23821,   228,   234,   168,   230,   246, 35975,   112,   167,\n",
      "           102,   112,  6407,    11, 23821,   243,   226, 46695,   230,   167,\n",
      "           102,   112, 10352,   167,    98,   120, 31619,   108,   246,   169,\n",
      "           247,   246, 47991,   102, 46695,   230, 46695,    97,   526, 15931,\n",
      "           198,   220,   220,   220,   611,   299, 19841,   352,    25,   198,\n",
      "           220,   220,   220,   220,   220,   220,   220,  1441, 10352,   198,\n",
      "           220,   220,   220,   611,   299, 19841,   513,    25,   198,   220,\n",
      "           220,   220,   220,   220,   220,   220,  1441,  6407,   198,   220,\n",
      "           220,   220,   611,   299,  4064,   362,  6624,   657,   393,   299,\n",
      "          4064,   513,  6624,   657,    25,   198,   220,   220,   220,   220,\n",
      "           220,   220,   220,  1441, 10352,   198,   220,   220,   220,  1312,\n",
      "           796,   642,   198,   220,   220,   220,   981,  1312,  1635,  1312,\n",
      "         19841,   299,    25,   198,   220,   220,   220,   220,   220,   220,\n",
      "           220,   611,   299,  4064,  1312,  6624,   657,   393,   299,  4064,\n",
      "           357,    72,  1343,   362,     8,  6624,   657,    25,   198,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1441, 10352,   198,   220,   220,   220,   220,   220,   220,   220,\n",
      "          1312, 15853,   718,   198,   220,   220,   220,  1441,  6407,   198,\n",
      "           198,     2, 23821,  8955,   168,   248,   102, 23821,   246,   230,\n",
      "           168,   233,   250,    25,   198,     2,   361, 11593,  3672,   834,\n",
      "          6624,   366,   834, 12417,   834,  1298,   198,     2,   220,   220,\n",
      "           220,   997,   796,  2808,   198,     2,   220,   220,   220,  3601,\n",
      "             7,    69,     1,    90, 22510,    92,   167,   232,   242, 23821,\n",
      "           228,   234,   168,   230,   246, 35975,   116,   166,   108,   222,\n",
      "           168,   248,   242,    30,  1391,   271,    62, 35505,     7, 22510,\n",
      "         38165,  4943,   628,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 패딩 토큰 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 코드 토큰화\n",
    "tokenized_codes = [tokenizer(code, return_tensors='pt', truncation=True, padding=True) for code in codes]\n",
    "\n",
    "# 결과 확인\n",
    "for i, tokenized_code in enumerate(tokenized_codes):\n",
    "    print(f\"Tokenized Code {i+1}:\")\n",
    "    print(tokenized_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b134afa7-9d9c-409f-9a11-594e98b16db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "class DeepSeekCoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length):\n",
    "        super(DeepSeekCoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "\n",
    "        # Decoder-Only Transformer (GPT 계열)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "\n",
    "        # 임베딩 + 포지셔널 인코딩\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:, :seq_length, :]\n",
    "\n",
    "        # Transformer Decoder (AutoRegressive 방식)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_length).to(input_ids.device)\n",
    "        x = self.decoder(x, x, tgt_mask=tgt_mask)\n",
    "\n",
    "        # 최종 출력\n",
    "        output = self.fc_out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 512\n",
    "\n",
    "model = DeepSeekCoder(vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da08cf00-767b-4c34-9ca6-473792d56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 패딩 토큰 ID (토크나이저에 따라 다를 수 있음, 예: 50256, 0 등)\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f750f93-365a-43fa-9613-eb5eb10c06fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|fim▁begin|>def merge_sort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    mid = len(arr) // 2\\n    left = merge_sort(arr[:mid])\\n    right = merge_sort(arr[mid:])\\n    return merge(left, right)\\n<|fim▁end|>',\n",
       " '<|fim▁begin|>def binary_search(arr, target):\\n    low, high = 0, len(arr) - 1\\n    <|fim▁hole|>\\n<|fim▁end|>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f8d4db9-b81c-426c-b642-3f30c3e2cc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  198,  4299,   751,     7,    64,    11,   275,  2599,   198,   220,\n",
      "           220,   220,  1441,   257,  1343,   275],\n",
      "        [  198,  4871, 16320, 18843,   602,    25,   198,   220,   220,   220,\n",
      "          2488, 12708, 24396,   198,   220,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# 이미 tokenized_codes 는 \"토큰화된 결과\" (list of dict) 임에도\n",
    "example_encodings = tokenizer(\n",
    "    codes,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=16,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# example_encodings = tokenizer(\n",
    "#     codes,\n",
    "#     padding=True,        # 최대 길이에 맞춰 짧은 문장을 0(또는 PAD)으로 채움\n",
    "#     truncation=True,     # 너무 긴 문장은 max_length로 잘라냄\n",
    "#     max_length=16,       # 예시로 16. 실제론 원하는 크기로 조절\n",
    "#     return_tensors=\"pt\"  # 곧바로 PyTorch 텐서 형태로 반환\n",
    "# )\n",
    "# 2) Dataset 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# 3) Dataset & DataLoader\n",
    "dataset = TextDataset(example_encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26d5a7d6-1521-4195-bc9b-f8ff0286e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 2\n",
      "[0] Type: str, Value: input_ids\n",
      "[1] Type: str, Value: attention_mask\n",
      "Dictionary Overview: 2 keys\n",
      "[0] Key: input_ids, Type: Tensor, Value: tensor([[  198,  4299,   751,     7,    64,    11,   275,  2599,   198,   220,\n",
      "           220,   220,  1441,   257,  1343,   275],\n",
      "        [  198,  4871, 16320, 18843,   602,    25,   198,   220,   22...\n",
      "[1] Key: attention_mask, Type: Tensor, Value: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "show \n",
      "     ----------------------------- \n",
      "  None\n"
     ]
    }
   ],
   "source": [
    "import dlpack_v1 as dl\n",
    "for batch in dataloader:\n",
    "    dl.display_items(batch)\n",
    "    #print(batch)\n",
    "    print('show \\n \\\n",
    "    ----------------------------- \\n ' , dl.show(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d44ff0-d630-4fd0-a008-17570f6cd451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "934834b4-a278-435b-96ee-3e067ca9de2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400] - Loss: 10.0870\n",
      "Epoch [2/400] - Loss: 8.1197\n",
      "Epoch [3/400] - Loss: 7.1597\n",
      "Epoch [4/400] - Loss: 6.4479\n",
      "Epoch [5/400] - Loss: 5.6025\n",
      "Epoch [6/400] - Loss: 4.9691\n",
      "Epoch [7/400] - Loss: 4.2989\n",
      "Epoch [8/400] - Loss: 3.7212\n",
      "Epoch [9/400] - Loss: 3.0856\n",
      "Epoch [10/400] - Loss: 2.7578\n",
      "Epoch [11/400] - Loss: 2.3377\n",
      "Epoch [12/400] - Loss: 1.9739\n",
      "Epoch [13/400] - Loss: 1.7198\n",
      "Epoch [14/400] - Loss: 1.4346\n",
      "Epoch [15/400] - Loss: 1.2314\n",
      "Epoch [16/400] - Loss: 1.0534\n",
      "Epoch [17/400] - Loss: 0.9471\n",
      "Epoch [18/400] - Loss: 0.7880\n",
      "Epoch [19/400] - Loss: 0.7097\n",
      "Epoch [20/400] - Loss: 0.5650\n",
      "Epoch [21/400] - Loss: 0.5873\n",
      "Epoch [22/400] - Loss: 0.5272\n",
      "Epoch [23/400] - Loss: 0.5708\n",
      "Epoch [24/400] - Loss: 0.4978\n",
      "Epoch [25/400] - Loss: 0.3917\n",
      "Epoch [26/400] - Loss: 0.3358\n",
      "Epoch [27/400] - Loss: 0.3360\n",
      "Epoch [28/400] - Loss: 0.2669\n",
      "Epoch [29/400] - Loss: 0.2733\n",
      "Epoch [30/400] - Loss: 0.2339\n",
      "Epoch [31/400] - Loss: 0.2178\n",
      "Epoch [32/400] - Loss: 0.1935\n",
      "Epoch [33/400] - Loss: 0.1599\n",
      "Epoch [34/400] - Loss: 0.1480\n",
      "Epoch [35/400] - Loss: 0.1335\n",
      "Epoch [36/400] - Loss: 0.1497\n",
      "Epoch [37/400] - Loss: 0.1683\n",
      "Epoch [38/400] - Loss: 0.3114\n",
      "Epoch [39/400] - Loss: 0.3182\n",
      "Epoch [40/400] - Loss: 0.2643\n",
      "Epoch [41/400] - Loss: 0.1841\n",
      "Epoch [42/400] - Loss: 0.3513\n",
      "Epoch [43/400] - Loss: 0.2573\n",
      "Epoch [44/400] - Loss: 0.2525\n",
      "Epoch [45/400] - Loss: 0.2124\n",
      "Epoch [46/400] - Loss: 0.1723\n",
      "Epoch [47/400] - Loss: 0.1864\n",
      "Epoch [48/400] - Loss: 0.1568\n",
      "Epoch [49/400] - Loss: 0.1414\n",
      "Epoch [50/400] - Loss: 0.1298\n",
      "Epoch [51/400] - Loss: 0.1174\n",
      "Epoch [52/400] - Loss: 0.1091\n",
      "Epoch [53/400] - Loss: 0.0814\n",
      "Epoch [54/400] - Loss: 0.0814\n",
      "Epoch [55/400] - Loss: 0.0702\n",
      "Epoch [56/400] - Loss: 0.0612\n",
      "Epoch [57/400] - Loss: 0.0628\n",
      "Epoch [58/400] - Loss: 0.1098\n",
      "Epoch [59/400] - Loss: 0.0590\n",
      "Epoch [60/400] - Loss: 0.1497\n",
      "Epoch [61/400] - Loss: 0.1687\n",
      "Epoch [62/400] - Loss: 0.2223\n",
      "Epoch [63/400] - Loss: 0.1051\n",
      "Epoch [64/400] - Loss: 0.0883\n",
      "Epoch [65/400] - Loss: 0.1114\n",
      "Epoch [66/400] - Loss: 0.0735\n",
      "Epoch [67/400] - Loss: 0.0813\n",
      "Epoch [68/400] - Loss: 0.0720\n",
      "Epoch [69/400] - Loss: 0.0605\n",
      "Epoch [70/400] - Loss: 0.0636\n",
      "Epoch [71/400] - Loss: 0.0454\n",
      "Epoch [72/400] - Loss: 0.0434\n",
      "Epoch [73/400] - Loss: 0.0677\n",
      "Epoch [74/400] - Loss: 0.0450\n",
      "Epoch [75/400] - Loss: 0.0497\n",
      "Epoch [76/400] - Loss: 0.0653\n",
      "Epoch [77/400] - Loss: 0.1971\n",
      "Epoch [78/400] - Loss: 0.4559\n",
      "Epoch [79/400] - Loss: 0.4170\n",
      "Epoch [80/400] - Loss: 0.1572\n",
      "Epoch [81/400] - Loss: 0.2397\n",
      "Epoch [82/400] - Loss: 0.1423\n",
      "Epoch [83/400] - Loss: 0.2213\n",
      "Epoch [84/400] - Loss: 0.1658\n",
      "Epoch [85/400] - Loss: 0.0927\n",
      "Epoch [86/400] - Loss: 0.0998\n",
      "Epoch [87/400] - Loss: 0.1098\n",
      "Epoch [88/400] - Loss: 0.1088\n",
      "Epoch [89/400] - Loss: 0.0906\n",
      "Epoch [90/400] - Loss: 0.0620\n",
      "Epoch [91/400] - Loss: 0.1162\n",
      "Epoch [92/400] - Loss: 0.1097\n",
      "Epoch [93/400] - Loss: 0.0559\n",
      "Epoch [94/400] - Loss: 0.0777\n",
      "Epoch [95/400] - Loss: 0.0576\n",
      "Epoch [96/400] - Loss: 0.0423\n",
      "Epoch [97/400] - Loss: 0.0327\n",
      "Epoch [98/400] - Loss: 0.0385\n",
      "Epoch [99/400] - Loss: 0.0366\n",
      "Epoch [100/400] - Loss: 0.0415\n",
      "Epoch [101/400] - Loss: 0.0356\n",
      "Epoch [102/400] - Loss: 0.0252\n",
      "Epoch [103/400] - Loss: 0.0279\n",
      "Epoch [104/400] - Loss: 0.0243\n",
      "Epoch [105/400] - Loss: 0.0301\n",
      "Epoch [106/400] - Loss: 0.0305\n",
      "Epoch [107/400] - Loss: 0.0205\n",
      "Epoch [108/400] - Loss: 0.0233\n",
      "Epoch [109/400] - Loss: 0.0702\n",
      "Epoch [110/400] - Loss: 0.0659\n",
      "Epoch [111/400] - Loss: 0.2176\n",
      "Epoch [112/400] - Loss: 0.1407\n",
      "Epoch [113/400] - Loss: 0.1444\n",
      "Epoch [114/400] - Loss: 0.1505\n",
      "Epoch [115/400] - Loss: 0.1084\n",
      "Epoch [116/400] - Loss: 0.0475\n",
      "Epoch [117/400] - Loss: 0.0565\n",
      "Epoch [118/400] - Loss: 0.0929\n",
      "Epoch [119/400] - Loss: 0.0343\n",
      "Epoch [120/400] - Loss: 0.0312\n",
      "Epoch [121/400] - Loss: 0.0286\n",
      "Epoch [122/400] - Loss: 0.0296\n",
      "Epoch [123/400] - Loss: 0.0334\n",
      "Epoch [124/400] - Loss: 0.0187\n",
      "Epoch [125/400] - Loss: 0.0415\n",
      "Epoch [126/400] - Loss: 0.0772\n",
      "Epoch [127/400] - Loss: 0.0881\n",
      "Epoch [128/400] - Loss: 0.0732\n",
      "Epoch [129/400] - Loss: 0.0701\n",
      "Epoch [130/400] - Loss: 0.0468\n",
      "Epoch [131/400] - Loss: 0.0833\n",
      "Epoch [132/400] - Loss: 0.0365\n",
      "Epoch [133/400] - Loss: 0.0633\n",
      "Epoch [134/400] - Loss: 0.0578\n",
      "Epoch [135/400] - Loss: 0.0648\n",
      "Epoch [136/400] - Loss: 0.0312\n",
      "Epoch [137/400] - Loss: 0.0220\n",
      "Epoch [138/400] - Loss: 0.0232\n",
      "Epoch [139/400] - Loss: 0.0358\n",
      "Epoch [140/400] - Loss: 0.0289\n",
      "Epoch [141/400] - Loss: 0.0841\n",
      "Epoch [142/400] - Loss: 0.1734\n",
      "Epoch [143/400] - Loss: 0.0239\n",
      "Epoch [144/400] - Loss: 0.1256\n",
      "Epoch [145/400] - Loss: 0.0848\n",
      "Epoch [146/400] - Loss: 0.0271\n",
      "Epoch [147/400] - Loss: 0.0409\n",
      "Epoch [148/400] - Loss: 0.0514\n",
      "Epoch [149/400] - Loss: 0.1053\n",
      "Epoch [150/400] - Loss: 0.0287\n",
      "Epoch [151/400] - Loss: 0.0306\n",
      "Epoch [152/400] - Loss: 0.0357\n",
      "Epoch [153/400] - Loss: 0.0197\n",
      "Epoch [154/400] - Loss: 0.0578\n",
      "Epoch [155/400] - Loss: 0.0154\n",
      "Epoch [156/400] - Loss: 0.0137\n",
      "Epoch [157/400] - Loss: 0.0154\n",
      "Epoch [158/400] - Loss: 0.0127\n",
      "Epoch [159/400] - Loss: 0.0216\n",
      "Epoch [160/400] - Loss: 0.0826\n",
      "Epoch [161/400] - Loss: 0.4453\n",
      "Epoch [162/400] - Loss: 0.0388\n",
      "Epoch [163/400] - Loss: 0.2144\n",
      "Epoch [164/400] - Loss: 0.1391\n",
      "Epoch [165/400] - Loss: 0.1163\n",
      "Epoch [166/400] - Loss: 0.1726\n",
      "Epoch [167/400] - Loss: 0.0792\n",
      "Epoch [168/400] - Loss: 0.0998\n",
      "Epoch [169/400] - Loss: 0.0841\n",
      "Epoch [170/400] - Loss: 0.0456\n",
      "Epoch [171/400] - Loss: 0.0365\n",
      "Epoch [172/400] - Loss: 0.0320\n",
      "Epoch [173/400] - Loss: 0.0365\n",
      "Epoch [174/400] - Loss: 0.0265\n",
      "Epoch [175/400] - Loss: 0.0359\n",
      "Epoch [176/400] - Loss: 0.0462\n",
      "Epoch [177/400] - Loss: 0.0394\n",
      "Epoch [178/400] - Loss: 0.1135\n",
      "Epoch [179/400] - Loss: 0.1178\n",
      "Epoch [180/400] - Loss: 0.0238\n",
      "Epoch [181/400] - Loss: 0.0507\n",
      "Epoch [182/400] - Loss: 0.0528\n",
      "Epoch [183/400] - Loss: 0.0520\n",
      "Epoch [184/400] - Loss: 0.0494\n",
      "Epoch [185/400] - Loss: 0.0182\n",
      "Epoch [186/400] - Loss: 0.0201\n",
      "Epoch [187/400] - Loss: 0.0271\n",
      "Epoch [188/400] - Loss: 0.0339\n",
      "Epoch [189/400] - Loss: 0.0331\n",
      "Epoch [190/400] - Loss: 0.0440\n",
      "Epoch [191/400] - Loss: 0.0237\n",
      "Epoch [192/400] - Loss: 0.1090\n",
      "Epoch [193/400] - Loss: 0.0225\n",
      "Epoch [194/400] - Loss: 0.0634\n",
      "Epoch [195/400] - Loss: 0.0129\n",
      "Epoch [196/400] - Loss: 0.1094\n",
      "Epoch [197/400] - Loss: 0.0365\n",
      "Epoch [198/400] - Loss: 0.1997\n",
      "Epoch [199/400] - Loss: 0.0472\n",
      "Epoch [200/400] - Loss: 0.1575\n",
      "Epoch [201/400] - Loss: 0.1142\n",
      "Epoch [202/400] - Loss: 0.0405\n",
      "Epoch [203/400] - Loss: 0.1015\n",
      "Epoch [204/400] - Loss: 0.0800\n",
      "Epoch [205/400] - Loss: 0.0235\n",
      "Epoch [206/400] - Loss: 0.1989\n",
      "Epoch [207/400] - Loss: 0.0430\n",
      "Epoch [208/400] - Loss: 0.0612\n",
      "Epoch [209/400] - Loss: 0.0374\n",
      "Epoch [210/400] - Loss: 0.0531\n",
      "Epoch [211/400] - Loss: 0.0452\n",
      "Epoch [212/400] - Loss: 0.0139\n",
      "Epoch [213/400] - Loss: 0.0689\n",
      "Epoch [214/400] - Loss: 0.0194\n",
      "Epoch [215/400] - Loss: 0.0282\n",
      "Epoch [216/400] - Loss: 0.0137\n",
      "Epoch [217/400] - Loss: 0.0090\n",
      "Epoch [218/400] - Loss: 0.0479\n",
      "Epoch [219/400] - Loss: 0.0088\n",
      "Epoch [220/400] - Loss: 0.0114\n",
      "Epoch [221/400] - Loss: 0.0448\n",
      "Epoch [222/400] - Loss: 0.0126\n",
      "Epoch [223/400] - Loss: 0.0516\n",
      "Epoch [224/400] - Loss: 0.0094\n",
      "Epoch [225/400] - Loss: 0.0547\n",
      "Epoch [226/400] - Loss: 0.0136\n",
      "Epoch [227/400] - Loss: 0.0920\n",
      "Epoch [228/400] - Loss: 0.0133\n",
      "Epoch [229/400] - Loss: 0.0860\n",
      "Epoch [230/400] - Loss: 0.0558\n",
      "Epoch [231/400] - Loss: 0.0177\n",
      "Epoch [232/400] - Loss: 0.0286\n",
      "Epoch [233/400] - Loss: 0.0206\n",
      "Epoch [234/400] - Loss: 0.0286\n",
      "Epoch [235/400] - Loss: 0.0136\n",
      "Epoch [236/400] - Loss: 0.0147\n",
      "Epoch [237/400] - Loss: 0.0110\n",
      "Epoch [238/400] - Loss: 0.0245\n",
      "Epoch [239/400] - Loss: 0.0156\n",
      "Epoch [240/400] - Loss: 0.0087\n",
      "Epoch [241/400] - Loss: 0.0126\n",
      "Epoch [242/400] - Loss: 0.0091\n",
      "Epoch [243/400] - Loss: 0.0073\n",
      "Epoch [244/400] - Loss: 0.0070\n",
      "Epoch [245/400] - Loss: 0.0213\n",
      "Epoch [246/400] - Loss: 0.0065\n",
      "Epoch [247/400] - Loss: 0.0079\n",
      "Epoch [248/400] - Loss: 0.0074\n",
      "Epoch [249/400] - Loss: 0.0113\n",
      "Epoch [250/400] - Loss: 0.0071\n",
      "Epoch [251/400] - Loss: 0.0233\n",
      "Epoch [252/400] - Loss: 0.0471\n",
      "Epoch [253/400] - Loss: 0.0442\n",
      "Epoch [254/400] - Loss: 0.0696\n",
      "Epoch [255/400] - Loss: 0.0165\n",
      "Epoch [256/400] - Loss: 0.0088\n",
      "Epoch [257/400] - Loss: 0.0458\n",
      "Epoch [258/400] - Loss: 0.0140\n",
      "Epoch [259/400] - Loss: 0.0140\n",
      "Epoch [260/400] - Loss: 0.0214\n",
      "Epoch [261/400] - Loss: 0.0110\n",
      "Epoch [262/400] - Loss: 0.0089\n",
      "Epoch [263/400] - Loss: 0.0088\n",
      "Epoch [264/400] - Loss: 0.0163\n",
      "Epoch [265/400] - Loss: 0.0078\n",
      "Epoch [266/400] - Loss: 0.0056\n",
      "Epoch [267/400] - Loss: 0.0058\n",
      "Epoch [268/400] - Loss: 0.0113\n",
      "Epoch [269/400] - Loss: 0.0057\n",
      "Epoch [270/400] - Loss: 0.0058\n",
      "Epoch [271/400] - Loss: 0.0059\n",
      "Epoch [272/400] - Loss: 0.0084\n",
      "Epoch [273/400] - Loss: 0.0055\n",
      "Epoch [274/400] - Loss: 0.0155\n",
      "Epoch [275/400] - Loss: 0.0050\n",
      "Epoch [276/400] - Loss: 0.0228\n",
      "Epoch [277/400] - Loss: 0.0050\n",
      "Epoch [278/400] - Loss: 0.0791\n",
      "Epoch [279/400] - Loss: 0.0052\n",
      "Epoch [280/400] - Loss: 0.0134\n",
      "Epoch [281/400] - Loss: 0.0056\n",
      "Epoch [282/400] - Loss: 0.0076\n",
      "Epoch [283/400] - Loss: 0.0128\n",
      "Epoch [284/400] - Loss: 0.0055\n",
      "Epoch [285/400] - Loss: 0.0078\n",
      "Epoch [286/400] - Loss: 0.0059\n",
      "Epoch [287/400] - Loss: 0.0158\n",
      "Epoch [288/400] - Loss: 0.0053\n",
      "Epoch [289/400] - Loss: 0.0055\n",
      "Epoch [290/400] - Loss: 0.0303\n",
      "Epoch [291/400] - Loss: 0.1030\n",
      "Epoch [292/400] - Loss: 0.0071\n",
      "Epoch [293/400] - Loss: 0.1652\n",
      "Epoch [294/400] - Loss: 0.1021\n",
      "Epoch [295/400] - Loss: 0.0848\n",
      "Epoch [296/400] - Loss: 0.0673\n",
      "Epoch [297/400] - Loss: 0.0308\n",
      "Epoch [298/400] - Loss: 0.0339\n",
      "Epoch [299/400] - Loss: 0.0323\n",
      "Epoch [300/400] - Loss: 0.0407\n",
      "Epoch [301/400] - Loss: 0.0177\n",
      "Epoch [302/400] - Loss: 0.0566\n",
      "Epoch [303/400] - Loss: 0.0711\n",
      "Epoch [304/400] - Loss: 0.0062\n",
      "Epoch [305/400] - Loss: 0.0228\n",
      "Epoch [306/400] - Loss: 0.0180\n",
      "Epoch [307/400] - Loss: 0.0211\n",
      "Epoch [308/400] - Loss: 0.0108\n",
      "Epoch [309/400] - Loss: 0.0490\n",
      "Epoch [310/400] - Loss: 0.0085\n",
      "Epoch [311/400] - Loss: 0.0169\n",
      "Epoch [312/400] - Loss: 0.0400\n",
      "Epoch [313/400] - Loss: 0.0085\n",
      "Epoch [314/400] - Loss: 0.0083\n",
      "Epoch [315/400] - Loss: 0.0074\n",
      "Epoch [316/400] - Loss: 0.0080\n",
      "Epoch [317/400] - Loss: 0.0123\n",
      "Epoch [318/400] - Loss: 0.0053\n",
      "Epoch [319/400] - Loss: 0.0055\n",
      "Epoch [320/400] - Loss: 0.0048\n",
      "Epoch [321/400] - Loss: 0.0080\n",
      "Epoch [322/400] - Loss: 0.0128\n",
      "Epoch [323/400] - Loss: 0.0156\n",
      "Epoch [324/400] - Loss: 0.0055\n",
      "Epoch [325/400] - Loss: 0.0045\n",
      "Epoch [326/400] - Loss: 0.0061\n",
      "Epoch [327/400] - Loss: 0.0131\n",
      "Epoch [328/400] - Loss: 0.0119\n",
      "Epoch [329/400] - Loss: 0.0063\n",
      "Epoch [330/400] - Loss: 0.0385\n",
      "Epoch [331/400] - Loss: 0.0218\n",
      "Epoch [332/400] - Loss: 0.1279\n",
      "Epoch [333/400] - Loss: 0.0276\n",
      "Epoch [334/400] - Loss: 0.0360\n",
      "Epoch [335/400] - Loss: 0.0555\n",
      "Epoch [336/400] - Loss: 0.0113\n",
      "Epoch [337/400] - Loss: 0.0432\n",
      "Epoch [338/400] - Loss: 0.0376\n",
      "Epoch [339/400] - Loss: 0.0174\n",
      "Epoch [340/400] - Loss: 0.0252\n",
      "Epoch [341/400] - Loss: 0.0162\n",
      "Epoch [342/400] - Loss: 0.0077\n",
      "Epoch [343/400] - Loss: 0.0057\n",
      "Epoch [344/400] - Loss: 0.0213\n",
      "Epoch [345/400] - Loss: 0.0101\n",
      "Epoch [346/400] - Loss: 0.0073\n",
      "Epoch [347/400] - Loss: 0.0207\n",
      "Epoch [348/400] - Loss: 0.0046\n",
      "Epoch [349/400] - Loss: 0.0139\n",
      "Epoch [350/400] - Loss: 0.0060\n",
      "Epoch [351/400] - Loss: 0.0037\n",
      "Epoch [352/400] - Loss: 0.0045\n",
      "Epoch [353/400] - Loss: 0.0099\n",
      "Epoch [354/400] - Loss: 0.0041\n",
      "Epoch [355/400] - Loss: 0.0043\n",
      "Epoch [356/400] - Loss: 0.0036\n",
      "Epoch [357/400] - Loss: 0.0077\n",
      "Epoch [358/400] - Loss: 0.0034\n",
      "Epoch [359/400] - Loss: 0.0045\n",
      "Epoch [360/400] - Loss: 0.0243\n",
      "Epoch [361/400] - Loss: 0.0086\n",
      "Epoch [362/400] - Loss: 0.0660\n",
      "Epoch [363/400] - Loss: 0.0356\n",
      "Epoch [364/400] - Loss: 0.0396\n",
      "Epoch [365/400] - Loss: 0.0115\n",
      "Epoch [366/400] - Loss: 0.0053\n",
      "Epoch [367/400] - Loss: 0.0263\n",
      "Epoch [368/400] - Loss: 0.0052\n",
      "Epoch [369/400] - Loss: 0.0240\n",
      "Epoch [370/400] - Loss: 0.0060\n",
      "Epoch [371/400] - Loss: 0.0066\n",
      "Epoch [372/400] - Loss: 0.0352\n",
      "Epoch [373/400] - Loss: 0.0086\n",
      "Epoch [374/400] - Loss: 0.0124\n",
      "Epoch [375/400] - Loss: 0.0499\n",
      "Epoch [376/400] - Loss: 0.0052\n",
      "Epoch [377/400] - Loss: 0.0047\n",
      "Epoch [378/400] - Loss: 0.0209\n",
      "Epoch [379/400] - Loss: 0.0515\n",
      "Epoch [380/400] - Loss: 0.0036\n",
      "Epoch [381/400] - Loss: 0.0146\n",
      "Epoch [382/400] - Loss: 0.0089\n",
      "Epoch [383/400] - Loss: 0.0052\n",
      "Epoch [384/400] - Loss: 0.0057\n",
      "Epoch [385/400] - Loss: 0.0046\n",
      "Epoch [386/400] - Loss: 0.0059\n",
      "Epoch [387/400] - Loss: 0.0049\n",
      "Epoch [388/400] - Loss: 0.0031\n",
      "Epoch [389/400] - Loss: 0.0032\n",
      "Epoch [390/400] - Loss: 0.0031\n",
      "Epoch [391/400] - Loss: 0.0036\n",
      "Epoch [392/400] - Loss: 0.0032\n",
      "Epoch [393/400] - Loss: 0.0046\n",
      "Epoch [394/400] - Loss: 0.0037\n",
      "Epoch [395/400] - Loss: 0.0030\n",
      "Epoch [396/400] - Loss: 0.0030\n",
      "Epoch [397/400] - Loss: 0.0031\n",
      "Epoch [398/400] - Loss: 0.0029\n",
      "Epoch [399/400] - Loss: 0.0028\n",
      "Epoch [400/400] - Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# device 설정 (GPU 사용 가능 시 GPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# (1) 모델 정의 (앞서 구성한 Decoder-Only 모델)\n",
    "# ------------------------------\n",
    "class DeepSeekCoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length):\n",
    "        super(DeepSeekCoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "\n",
    "        # Decoder-Only Transformer (GPT 계열)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        # 임베딩 + 포지셔널 인코딩\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:, :seq_length, :]\n",
    "        # Transformer는 [seq_len, batch_size, d_model] 형태를 요구함\n",
    "        x = x.transpose(0, 1)  # -> [seq_len, batch_size, d_model]\n",
    "        # Causal Mask (미래 토큰을 참조하지 않도록)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_length).to(input_ids.device)\n",
    "        x = self.decoder(x, x, tgt_mask=causal_mask)\n",
    "        # 다시 [batch_size, seq_len, d_model]로 변환\n",
    "        x = x.transpose(0, 1)\n",
    "        # 최종 출력층\n",
    "        logits = self.fc_out(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# ------------------------------\n",
    "# (2) 모델, 손실 함수, 옵티마이저 초기화\n",
    "# ------------------------------\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 512  # 실제 데이터 max_length와 맞춰야 함 (예시에서는 tokenizer max_length와 별개로 처리)\n",
    "\n",
    "model = DeepSeekCoder(vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length).to(device)\n",
    "\n",
    "# 패딩 토큰 ID (tokenizer.pad_token_id는 이미 eos_token로 설정되어 있음)\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ------------------------------\n",
    "# (3) 데이터셋 & DataLoader는 앞서 구성한 예시 사용\n",
    "#     (example_encodings는 tokenizer(codes, ...) 를 통해 얻은 결과)\n",
    "# ------------------------------\n",
    "# 이미 아래와 같이 example_encodings를 생성하였음:\n",
    "# example_encodings = tokenizer(codes, padding=True, truncation=True, max_length=16, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 및 DataLoader는 이미 정의되어 있으므로 그대로 사용:\n",
    "# class TextDataset(Dataset): ...  / dataset = TextDataset(example_encodings)\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# (4) 학습 루프: 100 epoch 동안 학습\n",
    "# ------------------------------\n",
    "epochs = 400\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        # 배치 데이터를 device로 이동\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # [batch_size, seq_len]\n",
    "        # attention_mask는 현재 모델 내부에서 사용하지 않으므로 생략해도 됩니다.\n",
    "        # 만약 attention_mask를 사용하고 싶다면, 모델 forward 인자에 추가\n",
    "\n",
    "        # 모델 forward pass\n",
    "        logits = model(input_ids)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # GPT 계열 학습은 다음 토큰 예측이므로,\n",
    "        # 입력 시퀀스를 한 칸씩 시프트해서 label을 구성합니다.\n",
    "        # 예) input_ids: [BOS, w1, w2, ..., w_{n-1}]\n",
    "        #     labels:    [w1, w2, ..., w_{n-1}, EOS]\n",
    "        shift_logits = logits[:, :-1, :].contiguous()  # [batch_size, seq_len-1, vocab_size]\n",
    "        shift_labels = input_ids[:, 1:].contiguous()     # [batch_size, seq_len-1]\n",
    "\n",
    "        # 손실 계산: logits을 (배치*시퀀스, vocab_size)로 펼치고, labels도 맞춤\n",
    "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                         shift_labels.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f09cf955-3059-4282-a882-10b1de910331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      " def quick_sort(arr):\n",
      "     if n == 0:\n",
      "  if n == 0:\n",
      "    if n == 0:\n",
      "   if n == 0:\n",
      "    if n == 0:\n",
      "    if n == 0:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    model.eval()  # 추론 모드로 전환\n",
    "    # 프롬프트를 토큰화 (padding, truncation 필요시 옵션 추가)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=tokenizer.model_max_length).input_ids.to(device)\n",
    "    \n",
    "    # 생성 결과를 저장할 변수 (초기 입력)\n",
    "    generated = input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # 모델에 전체 시퀀스를 넣어 다음 토큰 로짓 얻기\n",
    "            outputs = model(generated)  # [batch_size, seq_len, vocab_size]\n",
    "            next_token_logits = outputs[:, -1, :]  # 마지막 토큰에 대한 로짓\n",
    "            \n",
    "            # Greedy decoding: 가장 확률이 높은 토큰 선택\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # [batch_size, 1]\n",
    "            \n",
    "            # 생성 시퀀스에 이어 붙이기\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "            # 만약 EOS 토큰이 생성되면 중단 (tokenizer.eos_token_id 사용)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # 생성된 토큰 시퀀스를 문자열로 디코딩\n",
    "    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# 예시 사용:\n",
    "prompt = \"def quick_sort(arr):\\n    \"\n",
    "generated_code = generate_text(model, tokenizer, prompt, max_length=50)\n",
    "print(\"Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd765c-287a-43f6-8225-e4f590238beb",
   "metadata": {},
   "source": [
    "400 epoch 의 deepseek 의 fim 사용안한 gpt급의 기본 구현 결과\n",
    "```\n",
    "Generated Code:\n",
    " def quick_sort(arr):\n",
    "     if n == 0:\n",
    "  if n == 0:\n",
    "    if n == 0:\n",
    "   if n == 0:\n",
    "    if n == 0:\n",
    "    if n == 0:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ee2fb-f7ef-4a8e-9fd5-b6bdad5d8c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb162b-0544-4736-8363-c11a48b7320d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89745d61-68a0-4ffc-bf46-48276e3b013b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Loss: 9.4292\n",
      "Epoch [2/300] - Loss: 6.8868\n",
      "Epoch [3/300] - Loss: 6.5959\n",
      "Epoch [4/300] - Loss: 6.2313\n",
      "Epoch [5/300] - Loss: 5.8716\n",
      "Epoch [6/300] - Loss: 5.3339\n",
      "Epoch [7/300] - Loss: 5.2917\n",
      "Epoch [8/300] - Loss: 4.9719\n",
      "Epoch [9/300] - Loss: 4.8100\n",
      "Epoch [10/300] - Loss: 4.6060\n",
      "Epoch [11/300] - Loss: 4.4707\n",
      "Epoch [12/300] - Loss: 4.2431\n",
      "Epoch [13/300] - Loss: 3.9831\n",
      "Epoch [14/300] - Loss: 3.7114\n",
      "Epoch [15/300] - Loss: 3.6108\n",
      "Epoch [16/300] - Loss: 3.2792\n",
      "Epoch [17/300] - Loss: 3.1954\n",
      "Epoch [18/300] - Loss: 3.0948\n",
      "Epoch [19/300] - Loss: 2.8356\n",
      "Epoch [20/300] - Loss: 2.7544\n",
      "Epoch [21/300] - Loss: 2.5046\n",
      "Epoch [22/300] - Loss: 2.3502\n",
      "Epoch [23/300] - Loss: 2.2220\n",
      "Epoch [24/300] - Loss: 2.0961\n",
      "Epoch [25/300] - Loss: 1.9740\n",
      "Epoch [26/300] - Loss: 1.9632\n",
      "Epoch [27/300] - Loss: 1.7870\n",
      "Epoch [28/300] - Loss: 1.7694\n",
      "Epoch [29/300] - Loss: 1.6047\n",
      "Epoch [30/300] - Loss: 1.5874\n",
      "Epoch [31/300] - Loss: 1.4866\n",
      "Epoch [32/300] - Loss: 1.4547\n",
      "Epoch [33/300] - Loss: 1.3677\n",
      "Epoch [34/300] - Loss: 1.2900\n",
      "Epoch [35/300] - Loss: 1.2510\n",
      "Epoch [36/300] - Loss: 1.2271\n",
      "Epoch [37/300] - Loss: 1.1545\n",
      "Epoch [38/300] - Loss: 1.0476\n",
      "Epoch [39/300] - Loss: 1.0270\n",
      "Epoch [40/300] - Loss: 0.9899\n",
      "Epoch [41/300] - Loss: 0.9162\n",
      "Epoch [42/300] - Loss: 0.9234\n",
      "Epoch [43/300] - Loss: 0.9200\n",
      "Epoch [44/300] - Loss: 0.8391\n",
      "Epoch [45/300] - Loss: 0.8236\n",
      "Epoch [46/300] - Loss: 0.8509\n",
      "Epoch [47/300] - Loss: 0.7830\n",
      "Epoch [48/300] - Loss: 0.7851\n",
      "Epoch [49/300] - Loss: 0.7522\n",
      "Epoch [50/300] - Loss: 0.7230\n",
      "Epoch [51/300] - Loss: 0.7152\n",
      "Epoch [52/300] - Loss: 0.7420\n",
      "Epoch [53/300] - Loss: 0.6807\n",
      "Epoch [54/300] - Loss: 0.6680\n",
      "Epoch [55/300] - Loss: 0.6710\n",
      "Epoch [56/300] - Loss: 0.6400\n",
      "Epoch [57/300] - Loss: 0.6235\n",
      "Epoch [58/300] - Loss: 0.6119\n",
      "Epoch [59/300] - Loss: 0.5641\n",
      "Epoch [60/300] - Loss: 0.5753\n",
      "Epoch [61/300] - Loss: 0.5727\n",
      "Epoch [62/300] - Loss: 0.5757\n",
      "Epoch [63/300] - Loss: 0.5264\n",
      "Epoch [64/300] - Loss: 0.5493\n",
      "Epoch [65/300] - Loss: 0.5573\n",
      "Epoch [66/300] - Loss: 0.5055\n",
      "Epoch [67/300] - Loss: 0.5052\n",
      "Epoch [68/300] - Loss: 0.4988\n",
      "Epoch [69/300] - Loss: 0.4831\n",
      "Epoch [70/300] - Loss: 0.4705\n",
      "Epoch [71/300] - Loss: 0.4747\n",
      "Epoch [72/300] - Loss: 0.5023\n",
      "Epoch [73/300] - Loss: 0.4923\n",
      "Epoch [74/300] - Loss: 0.4693\n",
      "Epoch [75/300] - Loss: 0.5063\n",
      "Epoch [76/300] - Loss: 0.4727\n",
      "Epoch [77/300] - Loss: 0.4598\n",
      "Epoch [78/300] - Loss: 0.4112\n",
      "Epoch [79/300] - Loss: 0.4348\n",
      "Epoch [80/300] - Loss: 0.4016\n",
      "Epoch [81/300] - Loss: 0.3880\n",
      "Epoch [82/300] - Loss: 0.3943\n",
      "Epoch [83/300] - Loss: 0.3990\n",
      "Epoch [84/300] - Loss: 0.3458\n",
      "Epoch [85/300] - Loss: 0.4616\n",
      "Epoch [86/300] - Loss: 0.3857\n",
      "Epoch [87/300] - Loss: 0.5256\n",
      "Epoch [88/300] - Loss: 0.4772\n",
      "Epoch [89/300] - Loss: 0.4658\n",
      "Epoch [90/300] - Loss: 0.4331\n",
      "Epoch [91/300] - Loss: 0.4328\n",
      "Epoch [92/300] - Loss: 0.4234\n",
      "Epoch [93/300] - Loss: 0.4313\n",
      "Epoch [94/300] - Loss: 0.4081\n",
      "Epoch [95/300] - Loss: 0.3824\n",
      "Epoch [96/300] - Loss: 0.3865\n",
      "Epoch [97/300] - Loss: 0.3748\n",
      "Epoch [98/300] - Loss: 0.3263\n",
      "Epoch [99/300] - Loss: 0.3594\n",
      "Epoch [100/300] - Loss: 0.3321\n",
      "Epoch [101/300] - Loss: 0.3149\n",
      "Epoch [102/300] - Loss: 0.2916\n",
      "Epoch [103/300] - Loss: 0.3082\n",
      "Epoch [104/300] - Loss: 0.3083\n",
      "Epoch [105/300] - Loss: 0.3619\n",
      "Epoch [106/300] - Loss: 0.2842\n",
      "Epoch [107/300] - Loss: 0.3555\n",
      "Epoch [108/300] - Loss: 0.3090\n",
      "Epoch [109/300] - Loss: 0.3273\n",
      "Epoch [110/300] - Loss: 0.3023\n",
      "Epoch [111/300] - Loss: 0.3038\n",
      "Epoch [112/300] - Loss: 0.3276\n",
      "Epoch [113/300] - Loss: 0.4408\n",
      "Epoch [114/300] - Loss: 0.4200\n",
      "Epoch [115/300] - Loss: 0.3922\n",
      "Epoch [116/300] - Loss: 0.3375\n",
      "Epoch [117/300] - Loss: 0.3669\n",
      "Epoch [118/300] - Loss: 0.2901\n",
      "Epoch [119/300] - Loss: 0.2890\n",
      "Epoch [120/300] - Loss: 0.2654\n",
      "Epoch [121/300] - Loss: 0.2278\n",
      "Epoch [122/300] - Loss: 0.2278\n",
      "Epoch [123/300] - Loss: 0.1916\n",
      "Epoch [124/300] - Loss: 0.2547\n",
      "Epoch [125/300] - Loss: 0.1916\n",
      "Epoch [126/300] - Loss: 0.2053\n",
      "Epoch [127/300] - Loss: 0.1940\n",
      "Epoch [128/300] - Loss: 0.1912\n",
      "Epoch [129/300] - Loss: 0.1752\n",
      "Epoch [130/300] - Loss: 0.1611\n",
      "Epoch [131/300] - Loss: 0.1714\n",
      "Epoch [132/300] - Loss: 0.1562\n",
      "Epoch [133/300] - Loss: 0.1311\n",
      "Epoch [134/300] - Loss: 0.1487\n",
      "Epoch [135/300] - Loss: 0.1350\n",
      "Epoch [136/300] - Loss: 0.1113\n",
      "Epoch [137/300] - Loss: 0.1400\n",
      "Epoch [138/300] - Loss: 0.1400\n",
      "Epoch [139/300] - Loss: 0.1288\n",
      "Epoch [140/300] - Loss: 0.1278\n",
      "Epoch [141/300] - Loss: 0.1181\n",
      "Epoch [142/300] - Loss: 0.1426\n",
      "Epoch [143/300] - Loss: 0.1127\n",
      "Epoch [144/300] - Loss: 0.1616\n",
      "Epoch [145/300] - Loss: 0.1401\n",
      "Epoch [146/300] - Loss: 0.1831\n",
      "Epoch [147/300] - Loss: 0.2040\n",
      "Epoch [148/300] - Loss: 0.1812\n",
      "Epoch [149/300] - Loss: 0.1324\n",
      "Epoch [150/300] - Loss: 0.1503\n",
      "Epoch [151/300] - Loss: 0.1123\n",
      "Epoch [152/300] - Loss: 0.1229\n",
      "Epoch [153/300] - Loss: 0.1386\n",
      "Epoch [154/300] - Loss: 0.1283\n",
      "Epoch [155/300] - Loss: 0.1212\n",
      "Epoch [156/300] - Loss: 0.0840\n",
      "Epoch [157/300] - Loss: 0.0959\n",
      "Epoch [158/300] - Loss: 0.0757\n",
      "Epoch [159/300] - Loss: 0.0950\n",
      "Epoch [160/300] - Loss: 0.1058\n",
      "Epoch [161/300] - Loss: 0.1212\n",
      "Epoch [162/300] - Loss: 0.1211\n",
      "Epoch [163/300] - Loss: 0.1201\n",
      "Epoch [164/300] - Loss: 0.0913\n",
      "Epoch [165/300] - Loss: 0.1140\n",
      "Epoch [166/300] - Loss: 0.1951\n",
      "Epoch [167/300] - Loss: 0.1467\n",
      "Epoch [168/300] - Loss: 0.1298\n",
      "Epoch [169/300] - Loss: 0.1157\n",
      "Epoch [170/300] - Loss: 0.1177\n",
      "Epoch [171/300] - Loss: 0.1129\n",
      "Epoch [172/300] - Loss: 0.0744\n",
      "Epoch [173/300] - Loss: 0.1281\n",
      "Epoch [174/300] - Loss: 0.1271\n",
      "Epoch [175/300] - Loss: 0.0750\n",
      "Epoch [176/300] - Loss: 0.1368\n",
      "Epoch [177/300] - Loss: 0.0937\n",
      "Epoch [178/300] - Loss: 0.0978\n",
      "Epoch [179/300] - Loss: 0.0818\n",
      "Epoch [180/300] - Loss: 0.0924\n",
      "Epoch [181/300] - Loss: 0.0855\n",
      "Epoch [182/300] - Loss: 0.0829\n",
      "Epoch [183/300] - Loss: 0.0788\n",
      "Epoch [184/300] - Loss: 0.0681\n",
      "Epoch [185/300] - Loss: 0.0901\n",
      "Epoch [186/300] - Loss: 0.0692\n",
      "Epoch [187/300] - Loss: 0.0562\n",
      "Epoch [188/300] - Loss: 0.0628\n",
      "Epoch [189/300] - Loss: 0.0505\n",
      "Epoch [190/300] - Loss: 0.0536\n",
      "Epoch [191/300] - Loss: 0.0495\n",
      "Epoch [192/300] - Loss: 0.0490\n",
      "Epoch [193/300] - Loss: 0.0692\n",
      "Epoch [194/300] - Loss: 0.0541\n",
      "Epoch [195/300] - Loss: 0.0472\n",
      "Epoch [196/300] - Loss: 0.0676\n",
      "Epoch [197/300] - Loss: 0.0758\n",
      "Epoch [198/300] - Loss: 0.0652\n",
      "Epoch [199/300] - Loss: 0.0486\n",
      "Epoch [200/300] - Loss: 0.0594\n",
      "Epoch [201/300] - Loss: 0.0428\n",
      "Epoch [202/300] - Loss: 0.0673\n",
      "Epoch [203/300] - Loss: 0.0775\n",
      "Epoch [204/300] - Loss: 0.0744\n",
      "Epoch [205/300] - Loss: 0.0782\n",
      "Epoch [206/300] - Loss: 0.0781\n",
      "Epoch [207/300] - Loss: 0.0727\n",
      "Epoch [208/300] - Loss: 0.1282\n",
      "Epoch [209/300] - Loss: 0.0785\n",
      "Epoch [210/300] - Loss: 0.0905\n",
      "Epoch [211/300] - Loss: 0.0556\n",
      "Epoch [212/300] - Loss: 0.0675\n",
      "Epoch [213/300] - Loss: 0.0592\n",
      "Epoch [214/300] - Loss: 0.0446\n",
      "Epoch [215/300] - Loss: 0.0587\n",
      "Epoch [216/300] - Loss: 0.0369\n",
      "Epoch [217/300] - Loss: 0.0652\n",
      "Epoch [218/300] - Loss: 0.0563\n",
      "Epoch [219/300] - Loss: 0.0591\n",
      "Epoch [220/300] - Loss: 0.0735\n",
      "Epoch [221/300] - Loss: 0.0512\n",
      "Epoch [222/300] - Loss: 0.0444\n",
      "Epoch [223/300] - Loss: 0.0336\n",
      "Epoch [224/300] - Loss: 0.0476\n",
      "Epoch [225/300] - Loss: 0.0402\n",
      "Epoch [226/300] - Loss: 0.0497\n",
      "Epoch [227/300] - Loss: 0.0381\n",
      "Epoch [228/300] - Loss: 0.0277\n",
      "Epoch [229/300] - Loss: 0.0298\n",
      "Epoch [230/300] - Loss: 0.0253\n",
      "Epoch [231/300] - Loss: 0.0269\n",
      "Epoch [232/300] - Loss: 0.0252\n",
      "Epoch [233/300] - Loss: 0.0240\n",
      "Epoch [234/300] - Loss: 0.0268\n",
      "Epoch [235/300] - Loss: 0.0252\n",
      "Epoch [236/300] - Loss: 0.0226\n",
      "Epoch [237/300] - Loss: 0.0237\n",
      "Epoch [238/300] - Loss: 0.0235\n",
      "Epoch [239/300] - Loss: 0.0219\n",
      "Epoch [240/300] - Loss: 0.0210\n",
      "Epoch [241/300] - Loss: 0.0224\n",
      "Epoch [242/300] - Loss: 0.0235\n",
      "Epoch [243/300] - Loss: 0.0185\n",
      "Epoch [244/300] - Loss: 0.0247\n",
      "Epoch [245/300] - Loss: 0.0189\n",
      "Epoch [246/300] - Loss: 0.0205\n",
      "Epoch [247/300] - Loss: 0.0170\n",
      "Epoch [248/300] - Loss: 0.0231\n",
      "Epoch [249/300] - Loss: 0.0238\n",
      "Epoch [250/300] - Loss: 0.0268\n",
      "Epoch [251/300] - Loss: 0.0188\n",
      "Epoch [252/300] - Loss: 0.0244\n",
      "Epoch [253/300] - Loss: 0.0175\n",
      "Epoch [254/300] - Loss: 0.0255\n",
      "Epoch [255/300] - Loss: 0.0327\n",
      "Epoch [256/300] - Loss: 0.0327\n",
      "Epoch [257/300] - Loss: 0.0339\n",
      "Epoch [258/300] - Loss: 0.0319\n",
      "Epoch [259/300] - Loss: 0.0475\n",
      "Epoch [260/300] - Loss: 0.0222\n",
      "Epoch [261/300] - Loss: 0.0378\n",
      "Epoch [262/300] - Loss: 0.0273\n",
      "Epoch [263/300] - Loss: 0.0227\n",
      "Epoch [264/300] - Loss: 0.0257\n",
      "Epoch [265/300] - Loss: 0.0215\n",
      "Epoch [266/300] - Loss: 0.0289\n",
      "Epoch [267/300] - Loss: 0.0212\n",
      "Epoch [268/300] - Loss: 0.0479\n",
      "Epoch [269/300] - Loss: 0.0547\n",
      "Epoch [270/300] - Loss: 0.0587\n",
      "Epoch [271/300] - Loss: 0.0383\n",
      "Epoch [272/300] - Loss: 0.0261\n",
      "Epoch [273/300] - Loss: 0.0429\n",
      "Epoch [274/300] - Loss: 0.0315\n",
      "Epoch [275/300] - Loss: 0.0205\n",
      "Epoch [276/300] - Loss: 0.0257\n",
      "Epoch [277/300] - Loss: 0.0270\n",
      "Epoch [278/300] - Loss: 0.0581\n",
      "Epoch [279/300] - Loss: 0.0304\n",
      "Epoch [280/300] - Loss: 0.0411\n",
      "Epoch [281/300] - Loss: 0.0387\n",
      "Epoch [282/300] - Loss: 0.0415\n",
      "Epoch [283/300] - Loss: 0.0355\n",
      "Epoch [284/300] - Loss: 0.0296\n",
      "Epoch [285/300] - Loss: 0.0310\n",
      "Epoch [286/300] - Loss: 0.0273\n",
      "Epoch [287/300] - Loss: 0.0221\n",
      "Epoch [288/300] - Loss: 0.0191\n",
      "Epoch [289/300] - Loss: 0.0171\n",
      "Epoch [290/300] - Loss: 0.0175\n",
      "Epoch [291/300] - Loss: 0.0149\n",
      "Epoch [292/300] - Loss: 0.0245\n",
      "Epoch [293/300] - Loss: 0.0271\n",
      "Epoch [294/300] - Loss: 0.0236\n",
      "Epoch [295/300] - Loss: 0.0328\n",
      "Epoch [296/300] - Loss: 0.0244\n",
      "Epoch [297/300] - Loss: 0.0460\n",
      "Epoch [298/300] - Loss: 0.0212\n",
      "Epoch [299/300] - Loss: 0.0118\n",
      "Epoch [300/300] - Loss: 0.0181\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 토크나이저 로드 (FIM 토큰들이 포함되어 있다고 가정)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# EOS 토큰을 패딩 토큰으로 사용\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. 토큰화 (전체 코드를 한 번에 토큰화)\n",
    "example_encodings = tokenizer(\n",
    "    codes,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,  # 실제 학습에 맞게 길이를 조정하세요\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Dataset 정의\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "dataset = TextDataset(example_encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. 모델 정의 (GPT-스타일 Decoder-Only 모델)\n",
    "class DeepSeekCoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length):\n",
    "        super(DeepSeekCoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:, :seq_length, :]\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_length).to(input_ids.device)\n",
    "        x = self.decoder(x, x, tgt_mask=causal_mask)\n",
    "        x = x.transpose(0, 1)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 512\n",
    "\n",
    "model = DeepSeekCoder(vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length).to(device)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. 손실함수와 옵티마이저 설정\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. 학습 루프 (5 Epoch)\n",
    "epochs = 300\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # GPT 스타일 shift: 입력 시퀀스를 한 칸 밀어 label 구성\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                         shift_labels.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6. FIM 기능 활용: FIM 전용 프롬프트를 처리하는 generate_fim_text 함수\n",
    "def generate_fim_text(model, tokenizer, prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    FIM(Fill-In-The-Middle) 기능을 지원하는 생성 함수.\n",
    "    프롬프트 내에 FIM 토큰들 (<|fim▁begin|>, <|fim▁hole|>, <|fim▁end|>)가 포함되어 있으면,\n",
    "    모델은 해당 부분을 채워 넣도록 학습되었다고 가정합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 프롬프트를 토큰화 (여기서 FIM 토큰들도 그대로 유지)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                            max_length=tokenizer.model_max_length).input_ids.to(device)\n",
    "    \n",
    "    # 모델 generate 메서드를 활용 (여기서는 간단히 greedy decoding 사용)\n",
    "    outputs = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50, top_p=0.95)\n",
    "    # skip_special_tokens=False로 하여 FIM 토큰들이 어떻게 채워졌는지 확인 가능\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa72555d-1d18-4432-8c09-c81401bbb9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 가능하면 GPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33cb8ecc-2f4d-4a07-9291-e83b2410106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocab size: 50260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. 토크나이저 로드 및 특수 토큰 설정 ---\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# FIM 전용 토큰 추가 (이미 존재하지 않으면 추가)\n",
    "special_tokens = {\"additional_special_tokens\": [\"<|fim▁begin|>\", \"<|fim▁hole|>\", \"<|fim▁end|>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "\n",
    "# 업데이트된 vocab 크기를 사용합니다.\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Updated vocab size:\", vocab_size)\n",
    "\n",
    "# --- 2. FIM 전용 예제 데이터 및 정답 라벨 준비 ---\n",
    "# 예제 1: quick_sort 함수 예제 (FIM 영역 채우기)\n",
    "code_example1 = (\n",
    "    \"<|fim▁begin|>def quick_sort(arr):\\n\"\n",
    "    \"    if len(arr) <= 1:\\n\"\n",
    "    \"        return arr\\n\"\n",
    "    \"    pivot = arr[0]\\n\"\n",
    "    \"    left = [x for x in arr[1:] if x < pivot]\\n\"\n",
    "    \"    right = [x for x in arr[1:] if x >= pivot]\\n\"\n",
    "    \"    <|fim▁hole|>\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "label_example1_text = \"return quick_sort(left) + [pivot] + quick_sort(right)\"\n",
    "fim_label1 = tokenizer(label_example1_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# 예제 2: Calculator와 binary_search 함수 예제 (FIM 영역 채우기)\n",
    "code_example2 = (\n",
    "    \"<|fim▁begin|>class Calculator:\\n\"\n",
    "    \"    def __init__(self):\\n\"\n",
    "    \"        self.result = 0\\n\\n\"\n",
    "    \"    def add(self, a, b):\\n\"\n",
    "    \"        self.result = a + b\\n\"\n",
    "    \"        return self.result\\n\\n\"\n",
    "    \"    def subtract(self, a, b):\\n\"\n",
    "    \"        self.result = a - b\\n\"\n",
    "    \"        return self.result\\n\\n\"\n",
    "    \"def binary_search(arr, target):\\n\"\n",
    "    \"    low, high = 0, len(arr) - 1\\n\"\n",
    "    \"    <|fim▁hole|>\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "label_example2_text = (\n",
    "    \"while low <= high:\\n\"\n",
    "    \"    mid = (low + high) // 2\\n\"\n",
    "    \"    if arr[mid] == target:\\n\"\n",
    "    \"        return mid\\n\"\n",
    "    \"    elif arr[mid] < target:\\n\"\n",
    "    \"        low = mid + 1\\n\"\n",
    "    \"    else:\\n\"\n",
    "    \"        high = mid - 1\\n\"\n",
    "    \"return -1\"\n",
    ")\n",
    "fim_label2 = tokenizer(label_example2_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# 예제 3: factorial와 fibonacci 함수 예제 (FIM 영역 채우기)\n",
    "code_example3 = (\n",
    "    \"<|fim▁begin|>def factorial(n):\\n\"\n",
    "    \"    if n == 0:\\n\"\n",
    "    \"        return 1\\n\"\n",
    "    \"    else:\\n\"\n",
    "    \"        return n * factorial(n - 1)\\n\\n\"\n",
    "    \"def fibonacci(n):\\n\"\n",
    "    \"    \\\"\\\"\\\"n개의 피보나치 수열을 반환합니다.\\\"\\\"\\\"\\n\"\n",
    "    \"    if n <= 0:\\n\"\n",
    "    \"        return []\\n\"\n",
    "    \"    elif n == 1:\\n\"\n",
    "    \"        return [0]\\n\"\n",
    "    \"    seq = [0, 1]\\n\"\n",
    "    \"    <|fim▁hole|>\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "label_example3_text = (\n",
    "    \"while len(seq) < n:\\n\"\n",
    "    \"    seq.append(seq[-1] + seq[-2])\\n\"\n",
    "    \"return seq\"\n",
    ")\n",
    "fim_label3 = tokenizer(label_example3_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# 예제 4: is_prime 함수 예제 (FIM 영역 채우기)\n",
    "code_example4 = (\n",
    "    \"<|fim▁begin|>def is_prime(n):\\n\"\n",
    "    \"    \\\"\\\"\\\"n이 소수이면 True, 아니면 False를 반환합니다.\\\"\\\"\\\"\\n\"\n",
    "    \"    if n <= 1:\\n\"\n",
    "    \"        return False\\n\"\n",
    "    \"    if n <= 3:\\n\"\n",
    "    \"        return True\\n\"\n",
    "    \"    if n % 2 == 0 or n % 3 == 0:\\n\"\n",
    "    \"        return False\\n\"\n",
    "    \"    i = 5\\n\"\n",
    "    \"    <|fim▁hole|>\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "label_example4_text = (\n",
    "    \"while i * i <= n:\\n\"\n",
    "    \"    if n % i == 0 or n % (i + 2) == 0:\\n\"\n",
    "    \"        return False\\n\"\n",
    "    \"    i += 6\\n\"\n",
    "    \"return True\"\n",
    ")\n",
    "fim_label4 = tokenizer(label_example4_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# 예제 5: Matrix Multiplication 함수 예제 (FIM 영역 채우기)\n",
    "code_example5 = (\n",
    "    \"<|fim▁begin|>class MathOperations:\\n\"\n",
    "    \"    @staticmethod\\n\"\n",
    "    \"    def multiply(a, b):\\n\"\n",
    "    \"        return a * b\\n\\n\"\n",
    "    \"    @staticmethod\\n\"\n",
    "    \"    def divide(a, b):\\n\"\n",
    "    \"        if b == 0:\\n\"\n",
    "    \"            raise ValueError(\\\"Cannot divide by zero\\\")\\n\"\n",
    "    \"        return a / b\\n\\n\"\n",
    "    \"def matrix_multiply(A, B):\\n\"\n",
    "    \"    \\\"\\\"\\\"\\n\"\n",
    "    \"    두 행렬 A와 B의 곱을 계산합니다.\\n\"\n",
    "    \"    A: m x n 행렬, B: n x p 행렬\\n\"\n",
    "    \"    반환값: m x p 행렬\\n\"\n",
    "    \"    \\\"\\\"\\\"\\n\"\n",
    "    \"    m = len(A)\\n\"\n",
    "    \"    n = len(A[0])\\n\"\n",
    "    \"    p = len(B[0])\\n\\n\"\n",
    "    \"    # 결과 행렬을 0으로 초기화\\n\"\n",
    "    \"    result = [[0 for _ in range(p)] for _ in range(m)]\\n\\n\"\n",
    "    \"    <|fim▁hole|>\\n\\n\"\n",
    "    \"    return result\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "label_example5_text = (\n",
    "    \"for i in range(m):\\n\"\n",
    "    \"    for j in range(p):\\n\"\n",
    "    \"        for k in range(n):\\n\"\n",
    "    \"            result[i][j] += A[i][k] * B[k][j]\"\n",
    ")\n",
    "fim_label5 = tokenizer(label_example5_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "# 이제 예제들을 리스트로 모읍니다.\n",
    "codes = [code_example1, code_example2, code_example3, code_example4, code_example5]\n",
    "# 모든 예제에 대해 FIM 라벨을 제공하도록 수정 (예시: dummy 라벨)\n",
    "fim_labels = [\n",
    "    # 예제 1: quick_sort\n",
    "    tokenizer(\"return quick_sort(left) + [pivot] + quick_sort(right)\", add_special_tokens=False)[\"input_ids\"],\n",
    "    # 예제 2: binary_search\n",
    "    tokenizer(\n",
    "        \"while low <= high:\\n    mid = (low + high) // 2\\n    if arr[mid] == target:\\n        return mid\\n    elif arr[mid] < target:\\n        low = mid + 1\\n    else:\\n        high = mid - 1\\nreturn -1\", \n",
    "        add_special_tokens=False\n",
    "    )[\"input_ids\"],\n",
    "    # 예제 3: fibonacci\n",
    "    tokenizer(\"while len(seq) < n:\\n    seq.append(seq[-1] + seq[-2])\\nreturn seq\", add_special_tokens=False)[\"input_ids\"],\n",
    "    # 예제 4: is_prime\n",
    "    tokenizer(\"while i * i <= n:\\n    if n % i == 0 or n % (i + 2) == 0:\\n        return False\\n    i += 6\\nreturn True\", add_special_tokens=False)[\"input_ids\"],\n",
    "    # 예제 5: matrix_multiply\n",
    "    tokenizer(\"for i in range(m):\\n    for j in range(p):\\n        for k in range(n):\\n            result[i][j] += A[i][k] * B[k][j]\", add_special_tokens=False)[\"input_ids\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8419515-ea50-489c-9311-8b3c0b2a1554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7783,\n",
       "  2068,\n",
       "  62,\n",
       "  30619,\n",
       "  7,\n",
       "  9464,\n",
       "  8,\n",
       "  1343,\n",
       "  685,\n",
       "  79,\n",
       "  45785,\n",
       "  60,\n",
       "  1343,\n",
       "  2068,\n",
       "  62,\n",
       "  30619,\n",
       "  7,\n",
       "  3506,\n",
       "  8],\n",
       " [4514,\n",
       "  1877,\n",
       "  19841,\n",
       "  1029,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  3095,\n",
       "  796,\n",
       "  357,\n",
       "  9319,\n",
       "  1343,\n",
       "  1029,\n",
       "  8,\n",
       "  3373,\n",
       "  362,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  611,\n",
       "  5240,\n",
       "  58,\n",
       "  13602,\n",
       "  60,\n",
       "  6624,\n",
       "  2496,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1441,\n",
       "  3095,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1288,\n",
       "  361,\n",
       "  5240,\n",
       "  58,\n",
       "  13602,\n",
       "  60,\n",
       "  1279,\n",
       "  2496,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1877,\n",
       "  796,\n",
       "  3095,\n",
       "  1343,\n",
       "  352,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  2073,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1029,\n",
       "  796,\n",
       "  3095,\n",
       "  532,\n",
       "  352,\n",
       "  198,\n",
       "  7783,\n",
       "  532,\n",
       "  16],\n",
       " [4514,\n",
       "  18896,\n",
       "  7,\n",
       "  41068,\n",
       "  8,\n",
       "  1279,\n",
       "  299,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  33756,\n",
       "  13,\n",
       "  33295,\n",
       "  7,\n",
       "  41068,\n",
       "  58,\n",
       "  12,\n",
       "  16,\n",
       "  60,\n",
       "  1343,\n",
       "  33756,\n",
       "  58,\n",
       "  12,\n",
       "  17,\n",
       "  12962,\n",
       "  198,\n",
       "  7783,\n",
       "  33756],\n",
       " [4514,\n",
       "  1312,\n",
       "  1635,\n",
       "  1312,\n",
       "  19841,\n",
       "  299,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  611,\n",
       "  299,\n",
       "  4064,\n",
       "  1312,\n",
       "  6624,\n",
       "  657,\n",
       "  393,\n",
       "  299,\n",
       "  4064,\n",
       "  357,\n",
       "  72,\n",
       "  1343,\n",
       "  362,\n",
       "  8,\n",
       "  6624,\n",
       "  657,\n",
       "  25,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1441,\n",
       "  10352,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1312,\n",
       "  15853,\n",
       "  718,\n",
       "  198,\n",
       "  7783,\n",
       "  6407],\n",
       " [1640,\n",
       "  1312,\n",
       "  287,\n",
       "  2837,\n",
       "  7,\n",
       "  76,\n",
       "  2599,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  329,\n",
       "  474,\n",
       "  287,\n",
       "  2837,\n",
       "  7,\n",
       "  79,\n",
       "  2599,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  329,\n",
       "  479,\n",
       "  287,\n",
       "  2837,\n",
       "  7,\n",
       "  77,\n",
       "  2599,\n",
       "  198,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  220,\n",
       "  1255,\n",
       "  58,\n",
       "  72,\n",
       "  7131,\n",
       "  73,\n",
       "  60,\n",
       "  15853,\n",
       "  317,\n",
       "  58,\n",
       "  72,\n",
       "  7131,\n",
       "  74,\n",
       "  60,\n",
       "  1635,\n",
       "  347,\n",
       "  58,\n",
       "  74,\n",
       "  7131,\n",
       "  73,\n",
       "  60]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7d05457-4316-4a8b-ae94-a1fc6a0c260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input_ids shape: torch.Size([5, 325])\n",
      "Example labels shape: torch.Size([5, 325])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 입력 데이터 토큰화 (codes 리스트 전체)\n",
    "example_encodings = tokenizer(\n",
    "    codes,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# FIM 라벨 생성: 각 예시마다 FIM 영역에 해당하는 라벨을 채워 넣되,\n",
    "# 라벨이 없는 경우에는 전체를 -100으로 처리합니다.\n",
    "labels_list = []\n",
    "for i, enc in enumerate(example_encodings[\"input_ids\"]):\n",
    "    current_fim_label = fim_labels[i] if i < len(fim_labels) else None\n",
    "    if current_fim_label is None:\n",
    "        labels_list.append(torch.full_like(enc, -100))\n",
    "    else:\n",
    "        lab = torch.full_like(enc, -100)\n",
    "        # \"<|fim▁hole|>\" 토큰의 인덱스 찾기\n",
    "        hole_token_id = tokenizer.convert_tokens_to_ids(\"<|fim▁hole|>\")\n",
    "        hole_indices = (enc == hole_token_id).nonzero(as_tuple=False)\n",
    "        if hole_indices.numel() > 0:\n",
    "            hole_index = hole_indices[0].item()\n",
    "            fim_ids = torch.tensor(current_fim_label, dtype=torch.long)\n",
    "            end_pos = min(hole_index + 1 + len(fim_ids), enc.size(0))\n",
    "            lab[hole_index+1:end_pos] = fim_ids[:end_pos - (hole_index+1)]\n",
    "        labels_list.append(lab)\n",
    "\n",
    "labels_tensor = torch.stack(labels_list, dim=0)\n",
    "example_encodings[\"labels\"] = labels_tensor\n",
    "\n",
    "# 디버깅: 각 예시의 input_ids와 labels 범위 출력\n",
    "print(\"Example input_ids shape:\", example_encodings[\"input_ids\"].shape)\n",
    "print(\"Example labels shape:\", example_encodings[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a38335a3-37fe-4f5d-9fad-67c55bce5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch input_ids range: 7 50259\n"
     ]
    }
   ],
   "source": [
    "class FIMTextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "        self.labels = encodings[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "dataset = FIMTextDataset(example_encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 디버깅: 첫 배치 확인\n",
    "for batch in dataloader:\n",
    "    print(\"First batch input_ids range:\",\n",
    "          batch[\"input_ids\"].min().item(), batch[\"input_ids\"].max().item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f414334a-4650-414d-866b-4a9f93a6f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekCoderFIM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length):\n",
    "        super(DeepSeekCoderFIM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:, :seq_length, :]\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_length).to(input_ids.device)\n",
    "        x = self.decoder(x, x, tgt_mask=causal_mask)\n",
    "        x = x.transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 512\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DeepSeekCoderFIM(vocab_size, d_model, num_decoder_layers, nhead, dim_feedforward, max_seq_length).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df28db87-a4c5-4111-a64e-450cf5fcda1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids range: 7 50259\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    input_min = batch[\"input_ids\"].min().item()\n",
    "    input_max = batch[\"input_ids\"].max().item()\n",
    "    print(\"Batch input_ids range:\", input_min, input_max)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a286a07-b9f6-418c-a935-c9c164e559cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hole token id: 50258\n"
     ]
    }
   ],
   "source": [
    "hole_token_id = tokenizer.convert_tokens_to_ids(\"<|fim▁hole|>\")\n",
    "print(\"Hole token id:\", hole_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4910f7c-67a9-460e-93c8-ac689cb5023e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0541f3b7-b6e0-4e65-8eb2-b8e8232013f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|fim▁begin|>def quick_sort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[0]\\n    left = [x for x in arr[1:] if x < pivot]\\n    right = [x for x in arr[1:] if x >= pivot]\\n    <|fim▁hole|>\\n<|fim▁end|>',\n",
       " '<|fim▁begin|>class Calculator:\\n    def __init__(self):\\n        self.result = 0\\n\\n    def add(self, a, b):\\n        self.result = a + b\\n        return self.result\\n\\n    def subtract(self, a, b):\\n        self.result = a - b\\n        return self.result\\n\\ndef binary_search(arr, target):\\n    low, high = 0, len(arr) - 1\\n    <|fim▁hole|>\\n<|fim▁end|>',\n",
       " '<|fim▁begin|>def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\ndef fibonacci(n):\\n    \"\"\"n개의 피보나치 수열을 반환합니다.\"\"\"\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    seq = [0, 1]\\n    <|fim▁hole|>\\n<|fim▁end|>',\n",
       " '<|fim▁begin|>def is_prime(n):\\n    \"\"\"n이 소수이면 True, 아니면 False를 반환합니다.\"\"\"\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    <|fim▁hole|>\\n<|fim▁end|>',\n",
       " '<|fim▁begin|>class MathOperations:\\n    @staticmethod\\n    def multiply(a, b):\\n        return a * b\\n\\n    @staticmethod\\n    def divide(a, b):\\n        if b == 0:\\n            raise ValueError(\"Cannot divide by zero\")\\n        return a / b\\n\\ndef matrix_multiply(A, B):\\n    \"\"\"\\n    두 행렬 A와 B의 곱을 계산합니다.\\n    A: m x n 행렬, B: n x p 행렬\\n    반환값: m x p 행렬\\n    \"\"\"\\n    m = len(A)\\n    n = len(A[0])\\n    p = len(B[0])\\n\\n    # 결과 행렬을 0으로 초기화\\n    result = [[0 for _ in range(p)] for _ in range(m)]\\n\\n    <|fim▁hole|>\\n\\n    return result\\n<|fim▁end|>']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4efefba2-d6fa-4607-855d-2504a36727e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/400] - Loss: 7.563772\n",
      "Epoch [40/400] - Loss: 7.453516\n",
      "Epoch [60/400] - Loss: 7.450770\n",
      "Epoch [80/400] - Loss: 7.242742\n",
      "Epoch [100/400] - Loss: 7.144761\n",
      "Epoch [120/400] - Loss: 6.871487\n",
      "Epoch [140/400] - Loss: 7.119122\n",
      "Epoch [160/400] - Loss: 7.205934\n",
      "Epoch [180/400] - Loss: 6.899584\n",
      "Epoch [200/400] - Loss: 6.508456\n",
      "Epoch [220/400] - Loss: 6.915328\n",
      "Epoch [240/400] - Loss: 6.113220\n",
      "Epoch [260/400] - Loss: 6.824526\n",
      "Epoch [280/400] - Loss: 6.568932\n",
      "Epoch [300/400] - Loss: 6.507346\n",
      "Epoch [320/400] - Loss: 6.333518\n",
      "Epoch [340/400] - Loss: 6.526079\n",
      "Epoch [360/400] - Loss: 5.487428\n",
      "Epoch [380/400] - Loss: 6.232210\n",
      "Epoch [400/400] - Loss: 5.441832\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, labels=labels)\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 그래디언트 클리핑 적용 (max_norm 값은 1.0으로 설정, 필요 시 조정)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6717e1b-34cd-4744-8a76-a833f5927b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids range: 7 50259\n",
      "Batch labels range: -100 2837\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch input_ids range:\", batch[\"input_ids\"].min().item(), batch[\"input_ids\"].max().item())\n",
    "print(\"Batch labels range:\", batch[\"labels\"].min().item(), batch[\"labels\"].max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a8947a3-6e0f-41b4-ac71-bbfab556fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "      \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "      \n",
    "    return logits\n",
    "\n",
    "def generate_fim_text(model, tokenizer, prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                            max_length=tokenizer.model_max_length).input_ids.to(device)\n",
    "    generated = input_ids\n",
    "    for _ in range(max_length - generated.size(1)):\n",
    "        logits = model(generated)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        probabilities = torch.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    generated_text = tokenizer.decode(generated[0], skip_special_tokens=False)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2006180-fe29-4335-b4a0-21cbfe792a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하는줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bb2720a0-0b0c-40f9-ad43-83a970defc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIM Generated Code:\n",
      " <|fim▁begin|>def quick_sort(arr):\n",
      "    # 이 함수는 리스트를 정렬합니다. 주어진 리스트를 피벗을 기준으로 나누고,\n",
      "    # 재귀적으로 정렬합니다.\n",
      "    <|fim▁hole|>\n",
      "<|fim▁end|>\n"
     ]
    }
   ],
   "source": [
    "fim_prompt = (\n",
    "    \"<|fim▁begin|>def quick_sort(arr):\\n\"\n",
    "    \"    # 이 함수는 리스트를 정렬합니다. 주어진 리스트를 피벗을 기준으로 나누고,\\n\"\n",
    "    \"    # 재귀적으로 정렬합니다.\\n\"\n",
    "    \"    <|fim▁hole|>\\n\"\n",
    "    \"<|fim▁end|>\"\n",
    ")\n",
    "generated_code = generate_fim_text(model, tokenizer, fim_prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.95)\n",
    "print(\"FIM Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21811eef-920d-4cd4-99ee-2260310369fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep)",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
